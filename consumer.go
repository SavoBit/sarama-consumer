/*
  A simple kafka consumer-group client

  Copyright 2016 MistSys
*/

package consumer

import (
	"fmt"
	"log"
	"sort"
	"sync"
	"time"

	"github.com/Shopify/sarama"
	"github.com/mistsys/sarama-consumer/roundrobin"
)

const logging = true        // set to true to see log messages
const debug = false         // set to true to see debug messages
const per_msg_debug = false // set to true to see per-message debug messages

// low level logging function. Replace it with your own if desired before making any calls to the rest of the API
var Logf func(fmt string, args ...interface{}) = log.Printf

// logf logs a printf style message if log is enabled
func logf(fmt string, args ...interface{}) {
	if logging {
		Logf(fmt, args...)
	}
}

// dbgf logs a printf style message to somewhere reasonable if debug is enabled, and as efficiently as it can does nothing with any side effects if debug is disabled
func dbgf(fmt string, args ...interface{}) {
	if debug {
		Logf(fmt, args...)
	}
}

// msgf is similar to dbgf but used for per-message debug messages. since these are so numerous there's a separate compile-time flag to compile these out
// in addition, since the go 1.7 compiler isn't eliminating the call sufficiently to avoid allocating the []interface{} needed by "args...interface{}",
// we pass the pointer to the msg. That works and doesn't show up in the heap gc'ed trash profile (pprof -alloc_space)
func msgf(fmt string, msg *sarama.ConsumerMessage) {
	if per_msg_debug {
		Logf(fmt, msg.Topic, msg.Partition, msg.Offset)
	}
}

// minimum kafka API version required. Use this when constructing the sarama.Client's sarama.Config.MinVersion
var MinVersion = sarama.V0_9_0_0

// Error holds the errors generated by this package
type Error struct {
	Err       error    // underlying error
	Context   string   // description of the context surrounding the error
	Consumer  Consumer // nil, or Consumer which produced the error
	Topic     string   // "", or the topic which had the error
	Partition int32    // -1, or the partition which had the error
	cl        *client
}

func (err *Error) Error() string {
	if err.Topic != "" {
		if err.Partition != -1 {
			return fmt.Sprintf("consumer-group %q: Error %s, topic %q, partition %d: %s", err.cl.group_name, err.Context, err.Topic, err.Partition, err.Err)
		}
		return fmt.Sprintf("consumer-group %q: Error %s, topic %q: %s", err.cl.group_name, err.Context, err.Topic, err.Err)
	}
	return fmt.Sprintf("consumer-group %q: Error %s: %s", err.cl.group_name, err.Context, err.Err)
}

// Config is the configuration of a Client. Typically you'd create a default configuration with
// NewConfig, modify any fields of interest, and pass it to NewClient. Once passed to NewClient the
// Config must not be modified. (doing so leads to data races, and may caused bugs as well).
//
// In addition to this config, consumer's code also looks at the sarama.Config of the sarama.Client
// supplied to NewClient, especially at the Consumer.Offsets settings, Version, Metadata.Retry.Backoff,
// Metadata.RefreshFrequency and ChannelBufferSize.
type Config struct {
	Session struct {
		// The allowed session timeout for registered consumers (defaults to 30s).
		// Must be within the allowed server range.
		Timeout time.Duration
	}
	Rebalance struct {
		// The allowed rebalance timeout for registered consumers (defaults to 30s).
		// Must be within the allowed server range. Only functions if sarama.Config.Version >= 0.10.1
		// Otherwise Session.Timeout is used for rebalancing too.
		Timeout time.Duration
	}
	Heartbeat struct {
		// Interval between each heartbeat (defaults to 3s). It should be no more
		// than 1/3rd of the Group.Session.Timout setting
		Interval time.Duration
	}

	// the partitioner used to map partitions to consumer group members (defaults to a round-robin partitioner)
	Partitioner Partitioner

	// OffsetOutOfRange is the handler for sarama.ErrOffsetOutOfRange errors (defaults to sarama.OffsetNewest,nil).
	// Implementations must return the new starting offset in the partition, or an error. The sarama.Client is included
	// for convenience, since handling this might involve querying the partition's current offsets.
	OffsetOutOfRange OffsetOutOfRange

	// StartingOffset is a hook to allow modifying the starting offset when a Consumer begins to consume
	// a partition. (defaults to returning the last committed offset). Some consumers might want to jump
	// ahead to fresh messages. The sarama.Client is included for convenience, since handling this might involve
	// looking up a partition's offset by time. When no committed offset could be found -1 (sarama.OffsetNewest)
	// is passed in. An implementation might want to return client.Config().Consumer.Offsets.Initial in that case.
	StartingOffset StartingOffset
}

// types of the functions in the Config
type StartingOffset func(topic string, partition int32, committed_offset int64, client sarama.Client) (offset int64, err error)
type OffsetOutOfRange func(topic string, partition int32, client sarama.Client) (offset int64, err error)

// default implementation of Config.OffsetOutOfRange jumps to the current head of the partition.
func DefaultOffsetOutOfRange(topic string, partition int32, client sarama.Client) (int64, error) {
	return sarama.OffsetNewest, nil
}

// default implementation of Config.StartingOffset starts at the committed offset, or at sarama.Config.Consumer.Offsets.Initial
// if there is no committed offset.
func DefaultStartingOffset(topic string, partition int32, offset int64, client sarama.Client) (int64, error) {
	if offset == sarama.OffsetNewest {
		offset = client.Config().Consumer.Offsets.Initial
	}
	return offset, nil
}

// NewConfig constructs a default configuration.
func NewConfig() *Config {
	cfg := &Config{}
	cfg.Session.Timeout = 30 * time.Second
	cfg.Rebalance.Timeout = 30 * time.Second
	cfg.Heartbeat.Interval = 3 * time.Second
	cfg.Partitioner = roundrobin.RoundRobin
	cfg.OffsetOutOfRange = DefaultOffsetOutOfRange
	cfg.StartingOffset = DefaultStartingOffset
	return cfg
}

/*
  NewClient creates a new consumer group client on top of an existing
  sarama.Client.

  After this call the contents of config should be treated as read-only.
  config can be nil if the defaults are acceptable.

  The consumer group name is used to match this client with other
  instances running elsewhere, but connected to the same cluster
  of kafka brokers and using the same consumer group name.

  The supplied sarama.Client should have been constructed with a sarama.Config
  where sarama.Config.Version is >= consumer.MinVersion, and if full handling of
  ErrOffsetOutOfRange is desired, sarama.Config.Consumer.Return.Errors = true.

  In addition, this package uses the settings in sarama.Config.Consumer.Offsets
  and sarama.Config.Metadata.RefreshFrequency
*/
func NewClient(group_name string, config *Config, sarama_client sarama.Client) (Client, error) {

	cl := &client{
		client:     sarama_client,
		config:     config,
		group_name: group_name,

		errors: make(chan error),

		closed:       make(chan struct{}),
		add_consumer: make(chan add_consumer),
		rem_consumer: make(chan *consumer),
	}

	// start the client's manager goroutine
	rc := make(chan error)
	cl.wg.Add(1)
	go cl.run(rc)

	return cl, <-rc
}

/*
  Client is a kafaka client belonging to a consumer group. It is created by NewClient.
*/
type Client interface {
	// Consume returns a consumer of the given topic
	Consume(topic string) (Consumer, error)

	// Close closes the client. It must be called to shutdown
	// the client. It cleans up any unclosed topic Consumers created by this Client.
	// It does NOT close the inner sarama.Client.
	// Calling twice is NOT supported.
	Close()

	// Errors returns a channel which can (should) be monitored
	// for errors. callers should probably log or otherwise report
	// the returned errors. The channel closes when the client
	// is closed.
	Errors() <-chan error

	// TODO have a Status() method for debug/logging? Or is Errors() enough?
}

/*
  Consumer is a consumer of a topic.

  Messages from any partition assigned to this client arrive on the
  channel returned by Messages.

  Every message read from the Messages channel must be eventually passed
  to Done. Calling Done is the signal that that message has been consumed
  and the offset of that message can be committed back to kafka.

  Of course this requires that the message's Partition and Offset fields not
  be altered. Then again for what possible reason would you do such a thing?
*/
type Consumer interface {
	// Messages returns the channel of messages arriving from kafka. It always
	// returns the same result, so it is safe to call once and store the result.
	// Every message read from the channel should be passed to Done when processing
	// of the message is complete.
	Messages() <-chan *sarama.ConsumerMessage

	// Done indicates the processing of the message is complete, and its offset can
	// be committed to kafka. Calling Done twice with the same message, or with a
	// garbage message, can cause trouble.
	Done(*sarama.ConsumerMessage)

	// AsyncClose terminates the consumer cleanly. Callers can continue to read from
	// Messages channel until it is closed, or not, as they wish.
	// Calling Client.Close() performs a AsyncClose() on any remaining consumers.
	// Calling AsyncClose multiple times is permitted. Only the first call has any effect.
	// Never calling AsyncClose is also permitted. Client.Close() implies Consumer.AsyncClose.
	AsyncClose()

	// Close() terminates the consumer and waits for it to be finished committing the current
	// offsets to kafka. Calling twice happens to work at the moment, but let's not encourage it.
	Close()
}

/*
  Partitioner maps partitions to consumer group members.

  When the user wants control over the partitioning they should set
  Config.Partitioner to their implementation of Partitioner.
*/
type Partitioner interface {
	// PrepareJoin prepares a JoinGroupRequest given the topics supplied.
	// The simplest implementation would be something like
	//   join_req.AddGroupProtocolMetadata("<partitioner name>", &sarama.ConsumerGroupMemberMetadata{ Version: 1, Topics:  topics, })
	PrepareJoin(join_req *sarama.JoinGroupRequest, topics []string, current_assignments map[string][]int32)

	// Partition performs the partitioning. Given the requested
	// memberships from the JoinGroupResponse, it adds the results
	// to the SyncGroupRequest. Returning an error cancels everything.
	// The sarama.Client supplied to NewClient is included for convenince,
	// since performing the partitioning probably requires looking at each
	// topic's metadata, especially its list of partitions.
	Partition(*sarama.SyncGroupRequest, *sarama.JoinGroupResponse, sarama.Client) error

	// ParseSync parses the SyncGroupResponse and returns the map of topics
	// to partitions assigned to this client, or an error if the information
	// is not parsable.
	ParseSync(*sarama.SyncGroupResponse) (map[string][]int32, error)
}

// client implements the Client interface
type client struct {
	client     sarama.Client // the sarama client we were constructed from
	config     *Config       // our configuration (read-only)
	group_name string        // the client-group name

	errors chan error // channel over which asynchronous errors are reported

	closed chan struct{}  // channel which is closed to cause the client to shutdown
	wg     sync.WaitGroup // waitgroup which is done when the client is shutdown

	add_consumer chan add_consumer // command channel used to add a new consumer
	rem_consumer chan *consumer    // command channel used to remove an existing consumer
}

// Errors returns the channel over which asynchronous errors are observed.
func (cl *client) Errors() <-chan error { return cl.errors }

// add_consumer are the messages sent over the client.add_consumer channel
type add_consumer struct {
	con   *consumer
	reply chan<- error
}

func (cl *client) Consume(topic string) (Consumer, error) {
	sarama_consumer, err := sarama.NewConsumerFromClient(cl.client)
	if err != nil {
		return nil, cl.makeError("Consume sarama.NewConsumerFromClient", err)
	}

	chanbufsize := cl.client.Config().ChannelBufferSize // give ourselves some capacity once I know it runs right without any (capacity hides bugs :-)

	con := &consumer{
		cl:       cl,
		consumer: sarama_consumer,
		topic:    topic,

		messages: make(chan *sarama.ConsumerMessage, chanbufsize),

		closed: make(chan struct{}),
		exited: make(chan struct{}),

		assignments: make(chan *assignment, 1),
		commit_reqs: make(chan commit_req),

		restart_partitions: make(chan *partition),
		premessages:        make(chan *sarama.ConsumerMessage, chanbufsize),
		done:               make(chan *sarama.ConsumerMessage, chanbufsize),
	}

	reply := make(chan error)
	cl.add_consumer <- add_consumer{con, reply}
	err = <-reply
	if err != nil {
		// if an error is returned then it is up to us to close the sarama.Consumer
		_ = sarama_consumer.Close() // we already have an error to return. a 2nd one is too much
		return nil, err
	}
	return con, nil
}

// Close shutsdown the client and any remaining Consumers.
func (cl *client) Close() {
	// signal to cl.run() that it should exit
	dbgf("Close client of consumer-group %q", cl.group_name)
	close(cl.closed)
	// and wait for the shutdown to be complete
	cl.wg.Wait()
}

// run is a long lived goroutine which manages this client's membership in the consumer group.
func (cl *client) run(early_rc chan<- error) {
	defer cl.wg.Done()

	var member_id string                    // our group member id, assigned to us by kafka when we first make contact
	consumers := make(map[string]*consumer) // map of topic -> consumer
	var assignments map[string][]int32      // nil, or our currently assigned partitions (map of topic -> list of partitions)
	var wg sync.WaitGroup                   // waitgroup used to wait for all consumers to exit

	defer dbgf("consumer-group %q client exiting", cl.group_name)

	// add a consumer
	add := func(add add_consumer) {
		dbgf("client.run add(topic %q)", add.con.topic)
		if _, ok := consumers[add.con.topic]; ok {
			// topic already is being consumed. the way the standard kafka 0.9 group coordination works you cannot consume twice with the
			// same client. If you want to consume the same topic twice, use two Clients.
			add.reply <- cl.makeError("Consume", fmt.Errorf("topic %q is already being consumed", add.con.topic))
			return
		}
		consumers[add.con.topic] = add.con
		wg.Add(1)
		go add.con.run(&wg)
		add.reply <- nil
	}
	// remove a consumer
	rem := func(con *consumer) {
		dbgf("client.run rem(topic %q)", con.topic)
		existing_con := consumers[con.topic]
		if existing_con == con {
			delete(consumers, con.topic)
			delete(assignments, con.topic) // forget about the topic's partition assignment
		} // else it's some old consumer and we've already removed it
		// and let the consumer shutdown
		close(con.assignments)
		close(con.commit_reqs)
	}
	// shutdown the consumers. waits until they are all stopped. only call once and return afterwards, since it makes assumptions that hold only when it is used like that
	shutdown := func() {
		dbgf("client.run shutdown")
		// shutdown the remaining consumers
		for _, con := range consumers {
			con.AsyncClose()
		}
		// and consume any last rem_consumer messages from them
		go func() {
			wg.Wait()
			close(cl.rem_consumer)
		}()
		for con := range cl.rem_consumer {
			rem(con)
		}
		wg.Wait()
		// and shutdown the errors channel
		close(cl.errors)
	}

	pause := false
	refresh := false        // refresh the coordinating broker (after an I/O error or a ErrNotCoordinatorForConsumer)
	reopen := false         // reopen coordinating broker (after an I/O error)
	var coor *sarama.Broker // nil, or coordinating broker

	// loop rejoining the group each time the group reforms
join_loop:
	for {
		if pause {
			delay := cl.client.Config().Metadata.Retry.Backoff // TODO should we increase timeouts?
			dbgf("pausing %v", delay)
			// pause before continuing, so we don't fail continuously too fast
			timeout := time.After(delay)
		pause_loop:
			for {
				select {
				case <-timeout:
					break pause_loop
				case <-cl.closed:
					// shutdown the remaining consumers
					shutdown()
					return
				case a := <-cl.add_consumer:
					add(a)
				case r := <-cl.rem_consumer:
					rem(r)
				}
			}
			pause = false
		}

		if reopen {
			if coor != nil {
				dbgf("closing and reopening connection to coordinator %d %s", coor.ID(), coor.Addr())
				if ok, err := coor.Connected(); ok {
					err = coor.Close()
					if err != nil {
						cl.deliverError(fmt.Sprintf("Close()ing coordinating broker %d %s", coor.ID(), coor.Addr()), err)
					}
				} else if err != nil {
					// remote the earlier error
					cl.deliverError(fmt.Sprintf("past Open() of coordinating broker %d %s", coor.ID(), coor.Addr()), err)
				}

				err := coor.Open(cl.client.Config())
				if err != nil {
					cl.deliverError(fmt.Sprintf("re-Open()ing coordinating broker %d %s", coor.ID(), coor.Addr()), err)
				}
				// coor.Open() is asynchronous. We'll continue without waiting (without doing an coor.Connected() call)
				// because coor might not even be our coordinator anymore (and might not exist)
			}
			reopen = false

			// after reopening (successfully or not), always refresh the coordinator
			refresh = true
		}

		if refresh {
			dbgf("refreshing coordinating broker")

			// refresh the group coordinator (because sarama caches the result, and the cache must be manually refreshed by us when we decide an invalidate might be needed)
			err := cl.client.RefreshCoordinator(cl.group_name)
			if err != nil {
				err = cl.makeError("refreshing coordinating broker", err)
				if early_rc != nil {
					early_rc <- err
					return
				}
				cl.deliverError("", err)
				pause = true
				continue join_loop
			}
			refresh = false
		}

		// make contact with the kafka broker coordinating this group
		// NOTE: sarama keeps the result cached, so we aren't taking a round trip to the kafka brokers very time
		// (then again we need to manage sarama's cache too)
		var err error
		coor, err = cl.client.Coordinator(cl.group_name)
		if err != nil {
			err = cl.makeError("contacting coordinating broker "+coor.Addr(), err)
			if early_rc != nil {
				early_rc <- err
				return
			}
			cl.deliverError("", err)

			pause = true
			refresh = true
			continue join_loop
		}
		dbgf("Coordinator %v %v", coor.ID(), coor.Addr())

		// make sure we are connected to the broker
		if ok, err := coor.Connected(); !ok {
			err = cl.makeError("connecting coordinating broker "+coor.Addr(), err)
			if early_rc != nil {
				early_rc <- err
				return
			}
			cl.deliverError("", err)

			pause = true
			reopen = true
			continue join_loop
		}

		// join the group
		jreq := &sarama.JoinGroupRequest{
			GroupId:        cl.group_name,
			SessionTimeout: int32(cl.config.Session.Timeout / time.Millisecond),
			MemberId:       member_id,
			ProtocolType:   "consumer", // we implement the standard kafka 0.9 consumer protocol metadata
		}

		num_partitions := make(map[string]int, len(consumers))
		{ // prepare the join request
			var topics = make([]string, 0, len(consumers))
			var current_assignments = make(map[string][]int32, len(consumers))
			for topic := range consumers {
				topics = append(topics, topic)
				if a := assignments[topic]; a != nil && len(a) != 0 { // omit any topics for which we are not assigned a partition
					current_assignments[topic] = a
				}

				// and keep track of the # of partitions we saw before we joined
				partitions, err := cl.client.Partitions(topic)
				if err != nil {
					cl.deliverError(fmt.Sprintf("looking up partitions of topic %q", topic), err)
				} else {
					num_partitions[topic] = len(partitions)
				}
			}
			cl.config.Partitioner.PrepareJoin(jreq, topics, current_assignments)
		}

		dbgf("sending JoinGroupRequest %v", jreq)
		jresp, err := coor.JoinGroup(jreq)
		dbgf("received JoinGroupResponse %v, %v", jresp, err)
		if err != nil {
			// some I/O error happened; we should reopen and refresh the current coordinator
			reopen = true
		} else if jresp.Err != 0 {
			switch jresp.Err {
			case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
				refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
			case sarama.ErrUnknownMemberId:
				member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
			}
			err = jresp.Err
		}
		if err != nil {
			err = cl.makeError("joining group", err)
			// if it is still early (the 1st iteration of this loop) then return the error and bail out
			if early_rc != nil {
				early_rc <- err
				return
			}
			cl.deliverError("", err)

			pause = true
			continue join_loop
		}

		// we managed to get a successfull join-group response. that is far enough that basic communication is functioning
		// and we can declare that our early_rc is success and release the caller to NewClient
		if early_rc != nil {
			early_rc <- nil
			early_rc = nil
		}

		// save our member_id for next time we join, and the new generation id
		member_id = jresp.MemberId
		generation_id := jresp.GenerationId
		logf("consumer %q joining generation %d as member %q", cl.group_name, generation_id, member_id)

		// prepare a sync request
		sreq := &sarama.SyncGroupRequest{
			GroupId:      cl.group_name,
			GenerationId: generation_id,
			MemberId:     member_id,
		}

		// we have been chosen as the leader then we have to map the partitions
		if jresp.LeaderId == member_id {
			dbgf("leader is we")
			err := cl.config.Partitioner.Partition(sreq, jresp, cl.client)
			if err != nil {
				cl.deliverError("partitioning", err)
				// and rejoin (thus aborting this generation) since we can't partition it as needed
				pause = true
				continue join_loop
			}
		}

		// send SyncGroup
		dbgf("sending SyncGroupRequest %v", sreq)
		sresp, err := coor.SyncGroup(sreq)
		dbgf("received SyncGroupResponse %v, %v", sresp, err)
		if err != nil {
			reopen = true
		} else if sresp.Err != 0 {
			switch sresp.Err {
			case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
				refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
			case sarama.ErrUnknownMemberId:
				member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
			}
			err = sresp.Err
		}
		if err != nil {
			cl.deliverError("synchronizing group", err)
			pause = true
			continue join_loop
		}
		new_assignments, err := cl.config.Partitioner.ParseSync(sresp)
		if err != nil {
			cl.deliverError("decoding member assignments", err)
			pause = true
			continue join_loop
		}

		// keep track of which and how many partitions we are assigned
		assignments = new_assignments
		num_assigned_partitions := 0
		for _, parts := range assignments {
			num_assigned_partitions += len(parts)
		}
		logf("consumer %q assigned %d partitions; assignment: %v", cl.group_name, num_assigned_partitions, assignments)

		// save and distribute the new assignments to our topic consumers
		a := &assignment{
			generation_id: generation_id,
			coordinator:   coor,
			member_id:     member_id,
			assignments:   assignments,
		}
		for _, con := range consumers {
			select {
			case con.assignments <- a:
				// got it on the first try
			default:
				// con.assignment is full (it has a capacity of 1)
				// remove the stale assignment and place this one in its place
				select {
				case <-con.assignments:
					// we have room now (since we're the only code which writes to this channel)
					con.assignments <- a
				case con.assignments <- a:
					// in this case the consumer removed the stale assignment before we could
				}
			}
		}

		// start the heartbeat timer
		heartbeat_timer := time.After(cl.config.Heartbeat.Interval)
		// and the offset commit timer
		clconfig := cl.client.Config()
		var commit_timer <-chan time.Time
		if clconfig.Consumer.Offsets.CommitInterval > 0 {
			commit_timer = time.After(clconfig.Consumer.Offsets.CommitInterval)
		} // else don't commit periodically (we still commit when closing down)
		// and the metadata check timer
		var metadata_timer <-chan time.Time
		if clconfig.Metadata.RefreshFrequency > 0 {
			metadata_timer = time.After(clconfig.Metadata.RefreshFrequency)
		}

		// and loop, sending heartbeats until something happens and we need to rejoin (or exit)
		for {
			select {
			case <-cl.closed:
				// cl.Close() has been called; time to exit

				// shutdown any remaining consumers (causing them to sync their final offsets)
				shutdown()

				// and nicely leave the consumer group
				req := &sarama.LeaveGroupRequest{
					GroupId:  cl.group_name,
					MemberId: member_id,
				}
				dbgf("sending LeaveGroupRequest %v", req)
				resp, err := coor.LeaveGroup(req)
				dbgf("received LeaveGroupResponse %v, %v", resp, err)
				// note: we don't bother with the full error handling code, since we're exiting anyway
				if err == nil && resp.Err != 0 {
					err = resp.Err
				}
				if err != nil {
					cl.deliverError("leaving group", err)
				}

				// and we're done
				return

			case <-heartbeat_timer:
				// send a heartbeat
				req := &sarama.HeartbeatRequest{
					GroupId:      cl.group_name,
					MemberId:     member_id,
					GenerationId: generation_id,
				}
				dbgf("sending HeartbeatRequest %v", req)
				resp, err := coor.Heartbeat(req)
				dbgf("received HeartbeatResponse %v, %v", resp, err)
				if err != nil {
					reopen = true
				} else if resp.Err != 0 {
					switch resp.Err {
					case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
						refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
					case sarama.ErrUnknownMemberId:
						member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
					}
					err = resp.Err
				}
				if err != nil {
					cl.deliverError("heartbeating with "+coor.Addr(), err)
					// we've got heartbeat troubles of one kind or another; disconnect and reconnect
					continue join_loop
				}

				// and start the next heartbeat only after we get the response to this one
				// that way when the network or the broker are slow we back off.
				heartbeat_timer = time.After(cl.config.Heartbeat.Interval)

			case <-commit_timer:
				ocreq := &sarama.OffsetCommitRequest{
					ConsumerGroup:           cl.group_name,
					ConsumerGroupGeneration: generation_id,
					ConsumerID:              member_id,
					RetentionTime:           int64(clconfig.Consumer.Offsets.Retention / time.Millisecond),
					Version:                 2, // kafka 0.9.0 version, with RetentionTime
				}
				if clconfig.Consumer.Offsets.Retention == 0 { // note that this and the rounding math above means that if you wanted a retention time of 0 millseconds you could set Config.Offsets.RetentionTime to something < 1 ms, like 1 nanosecond
					ocreq.RetentionTime = -1 // use broker's value
				}
				var wg sync.WaitGroup
				resp := make(chan commit_resp, num_assigned_partitions) // allocating room for the responses helps the code run smoothly
				for _, con := range consumers {
					wg.Add(1)
					con.commit_reqs <- commit_req{resp, &wg}
				}
				go func(resp chan commit_resp, wg *sync.WaitGroup) {
					wg.Wait()
					close(resp)
				}(resp, &wg)
				for r := range resp {
					dbgf("ocreq.AddBlock(%q, %d, %d)", r.topic, r.partition, r.offset)
					ocreq.AddBlock(r.topic, r.partition, r.offset, 0, "")
				}
				dbgf("sending OffsetCommitRequest %v", ocreq)
				ocresp, err := coor.CommitOffset(ocreq)
				dbgf("received OffsetCommitResponse %v, %v", ocresp, err)
				// log any errors we got. there isn't much we can do about them
				if err != nil {
					cl.deliverError("committing offsets to "+coor.Addr(), err)
					reopen = true
				} else {
					for topic, partitions := range ocresp.Errors {
						for p, kerr := range partitions {
							if kerr != 0 {
								cl.deliverError(fmt.Sprintf("committing offset of topic %q partition %d", topic, p), kerr)
								switch kerr {
								case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
									refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
								case sarama.ErrUnknownMemberId:
									member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
								}
								err = kerr // any of the kerr's will do
							}
						}
					}
				}
				if err != nil {
					continue join_loop
				}

				commit_timer = time.After(clconfig.Consumer.Offsets.CommitInterval)

			case <-metadata_timer:
				dbgf("metadata timer")
				// the sarama.Client has refreshed its metadata within the interval
				// all we do is verify the number of partitions hasn't changed since we joined a topic
				// this is a local calculation, so no need for any fancy concurrency
				for topic := range consumers {
					partitions, err := cl.client.Partitions(topic)
					if err != nil {
						cl.deliverError(fmt.Sprintf("looking up the partitions of topic %q", topic), err)
						// and rejoin the groups
						continue join_loop
					}
					if len(partitions) != num_partitions[topic] {
						dbgf("num_partitions of topic %q changed from %d to %d; rejoining", topic, num_partitions[topic], len(partitions))
						// rejoin the new partition count (presumably some new partitions have been added, since you can't remove partitions from a running kafka broker)
						continue join_loop
					}
				}

				// this drifts slightly. is that good enough for this use case or must I use a time.Ticker? the worst that happens is an interval is skipped. That is ok, we'll
				// pick up the change in the next interval.
				metadata_timer = time.After(clconfig.Metadata.RefreshFrequency)

			case a := <-cl.add_consumer:
				add(a)
				// and rejoin so we can become a member of the new topic
				continue join_loop
			case r := <-cl.rem_consumer:
				rem(r)
				// and rejoin so we can be removed as member of the new topic
				continue join_loop
			}
		} // end of heartbeat loop
	} // end of join_loop
}

// makeError wraps err into a *Error, associating it with context
func (cl *client) makeError(context string, err error) *Error {
	return &Error{
		cl:        cl,
		Err:       err,
		Context:   context,
		Topic:     "",
		Partition: -1,
	}
}

// deliverError builds an error and delivers it to the channel returned by cl.Errors
func (cl *client) deliverError(context string, err error) {
	if context != "" {
		err = cl.makeError(context, err)
	}
	logf("%v", err)
	cl.errors <- err
}

// consumer implements the Consumer interface
type consumer struct {
	cl       *client
	consumer sarama.Consumer
	topic    string

	messages chan *sarama.ConsumerMessage

	closed     chan struct{} // channel which is closed when the consumer is AsyncClose()ed
	close_once sync.Once     // Once used to make sure we close only once
	exited     chan struct{} // channel which is closed when the consumer is far enough along in exiting that consumer.Close can return

	assignments chan *assignment // channel over which client.run sends consumer.run each generation's partition assignments
	commit_reqs chan commit_req  // channel over which client.run sends consumer.run request to fill out a OffsetCommitRequest

	restart_partitions chan *partition              // channel through which partition.run delivers partition restart [at new offset] requests
	premessages        chan *sarama.ConsumerMessage // channel through which partition.run delivers messages to consumer.run
	done               chan *sarama.ConsumerMessage // channel through which Done() returns messages
}

// commit_req is a request for a consumer to send back the client its part into a OffsetCommitRequest
type commit_req struct {
	resp chan<- commit_resp
	wg   *sync.WaitGroup
}

type commit_resp struct {
	topic     string
	partition int32
	offset    int64
	// if someday we make use of the timestamp and metadata fields we'd add them here
}

// assignment is this client's assigned partitions
type assignment struct {
	generation_id int32              // the current generation
	coordinator   *sarama.Broker     // the current client-group coordinating broker
	member_id     string             // the member_id assigned to us by the coordinator
	assignments   map[string][]int32 // map of topic -> list of partitions
}

// construct a *Error from this consumer
func (con *consumer) makeError(context string, err error) *Error {
	Err := con.cl.makeError(context, err)
	Err.Consumer = con
	Err.Topic = con.topic
	return Err
}

// construct and deliver an *Error from this consumer
func (con *consumer) deliverError(context string, partition int32, err error) {
	Err := con.makeError(context, err)
	Err.Partition = partition
	con.cl.deliverError("", Err)
}

func (con *consumer) Messages() <-chan *sarama.ConsumerMessage { return con.messages }

// close the consumer. it can safely be called multiple times
func (con *consumer) AsyncClose() {
	dbgf("AsyncClose consumer of topic %q", con.topic)
	con.close_once.Do(func() { close(con.closed) })
}

// close the consumer and wait
func (con *consumer) Close() {
	dbgf("Close consumer of topic %q", con.topic)
	con.AsyncClose() // initiate the shutdown
	<-con.exited     // and wait around until it is complete
}

// consumer goroutine coordinates consuming from multiple partitions in a topic
// NOTE WELL: this function must never do anything which would prevent it from processing message from client.run promptly.
// That means any channel I/O must include cases for con.assignments and con.commit_reqs.
func (con *consumer) run(wg *sync.WaitGroup) {
	var generation_id int32 // current generation
	var coor *sarama.Broker // current consumer group coordinating broker
	var member_id string    // our member id assigned by coor

	partitions := make(map[int32]*partition) // map of partition number -> partition consumer

	// shutdown the removed partitions, committing their last offset
	remove := func(removed []int32) {
		dbgf("consumer %q rem(%v)", con.topic, removed)
		if len(removed) == 0 {
			// nothing to do, and no point in sending an empty OffsetCommitRequest msg either
			return
		}
		clconfig := con.cl.client.Config()
		ocreq := &sarama.OffsetCommitRequest{
			ConsumerGroup:           con.cl.group_name,
			ConsumerGroupGeneration: generation_id,
			ConsumerID:              member_id,
			RetentionTime:           int64(clconfig.Consumer.Offsets.Retention / time.Millisecond),
			Version:                 2, // kafka 0.9.0 version, with RetentionTime
		}
		if clconfig.Consumer.Offsets.Retention == 0 { // note that this and the rounding math above means that if you wanted a retention time of 0 millseconds you could set Config.Offsets.RetentionTime to something < 1 ms, like 1 nanosecond
			ocreq.RetentionTime = -1 // use broker's value
		}
		for _, p := range removed {
			// stop consuming from partition p
			if part, ok := partitions[p]; ok {
				delete(partitions, p)
				part.consumer.Close()
				offset := part.oldest
				if len(part.buckets) != 0 {
					if part.buckets[0][0] == part.buckets[0][1] {
						// add to that the portion of the last block we know been completed (this is often useful when the traffic rate is low or a client shuts down cleanly, since it has probably cleanly returned all offsets we've delivered)
						offset += int64(part.buckets[0][1])
					} // else we don't know enough to commit any further
				}
				dbgf("ocreq.AddBlock(%q, %d, %d)", con.topic, p, offset)
				ocreq.AddBlock(con.topic, p, offset, 0, "")
				logf("consumer %q stopped consuming partition %d at offset %d", con.topic, p, offset)
			}
		}
		dbgf("sending OffsetCommitRequest %v", ocreq)
		ocresp, err := coor.CommitOffset(ocreq)
		dbgf("received OffsetCommitResponse %v, %v", ocresp, err)
		// log any errors we got. there isn't much we can do about them; the next consumer will start at an older offset
		if err != nil {
			con.deliverError("committing offsets", -1, err)
		} else {
			for _, partitions := range ocresp.Errors {
				for p, err := range partitions {
					if err != 0 {
						con.deliverError("committing offset", p, err)
					}
				}
			}
		}
	}

	// handle a commit request from client.run
	commit_req := func(c commit_req) {
		dbgf("consumer %q commit_req(%v)", con.topic, c)
		for p, partition := range partitions {
			offset := partition.oldest
			if len(partition.buckets) != 0 {
				if partition.buckets[0][0] == partition.buckets[0][1] {
					// add to that the portion of the last block we know been completed (this is useful when the message rate is slow)
					offset += int64(partition.buckets[0][1])
				} // else we don't know enough to commit any further
			}
			c.resp <- commit_resp{con.topic, p, offset}
		}
		c.wg.Done()
	}

	defer func() {
		if len(partitions) != 0 {
			// cleanup the remaining partition consumers
			removed := make([]int32, 0, len(partitions))
			for p := range partitions {
				removed = append(removed, p)
			}
			remove(removed)
		}

		con.consumer.Close()
		close(con.messages)

		// send ourselves to rem_consumer
	rem_loop:
		for {
			select {
			case c := <-con.commit_reqs:
				commit_req(c)
			case <-con.assignments:
				// ignore them, we're shutting down
			case con.cl.rem_consumer <- con:
				break rem_loop
			}
			// NOTE: <-con.done is not a case above because there is no good way to shut it down. it is never closed,
			// so we'd never know when it was drained. As a consequence, it's client.Done() which aborts when the
			// consumer closes, rather than us draining con.done
		}

		// drain any remaining requests from run.client, until
		// run.client closes the channels
		for c := range con.commit_reqs {
			commit_req(c)
		}
		for range con.assignments {
			// ignore them
		}

		dbgf("consumer of topic %q exiting", con.topic)
		close(con.exited)
		wg.Done()
	}()

	// handle a message sent to us via con.done
	done := func(msg *sarama.ConsumerMessage) {
		msgf("consumer done(%q:%d/%d)", msg)

		// a sanity check, just in case someone passes the msg into the wrong consumer
		if con.topic != msg.Topic {
			con.deliverError("Done()", -1, fmt.Errorf("BUG: Message from topic %q passed to consumer(%q).Done()", msg.Topic, con.topic))
			return
		}

		part := partitions[msg.Partition]
		if part == nil {
			dbgf("no partition %d in topic %q", msg.Partition, con.topic)
			return
		}
		delta := msg.Offset - part.oldest
		if delta < 0 {
			dbgf("stale message %q:%d/%d", msg.Topic, msg.Partition, msg.Offset)
			return
		}
		index := int(delta) >> 6 //  /64
		if index >= len(part.buckets) {
			dbgf("early message %d/%d", msg.Partition, msg.Offset)
			return
		}
		part.buckets[index][1]++
		if index == 0 {
			// we might have finished the oldest bucket
			for part.buckets[0] == [2]uint8{64, 64} {
				// the oldest bucket is complete; advance the last committed offset
				part.oldest += 64
				part.buckets = part.buckets[1:]
				if len(part.buckets) == 0 {
					break
				}
			}
		}
	}

	// handle an assignment message
	assignment := func(a *assignment) {
		dbgf("consumer %q assignment(%v)", con.topic, a)
		// see what has changed in the partition assignment of our topic
		new_partitions := a.assignments[con.topic]
		added, removed := difference(partitions, new_partitions)
		dbgf("consumer %q added %v, removed %v", con.topic, added, removed)

		// shutdown the partitions while we still belong to the previous generation
		remove(removed)

		// update the current generation and related info after committing the last offsets from the previous generation
		generation_id = a.generation_id
		coor = a.coordinator
		member_id = a.member_id

		if len(added) == 0 {
			// we're done early
			return
		}

		// the sarama-cluster code pauses here so that other consumers have time to sync their offsets. Should we do the same?
		// I've observed with kafka 0.9.0.1 that once the coordinator bumps the generation_id the client can't commit an offset with
		// the old id. So unless the client lies and sends generation_id+1 when it commits there is nothing it can commit, and there
		// is no point in waiting. So for now, no waiting.

		// fetch the last committed offsets of the new partitions
		oreq := &sarama.OffsetFetchRequest{
			ConsumerGroup: con.cl.group_name,
			Version:       1, // kafka 0.9.0 expects version 1 offset requests
		}
		for _, p := range added {
			oreq.AddPartition(con.topic, p)
		}

		dbgf("consumer %q sending OffsetFetchRequest %v", con.topic, oreq)
		oresp, err := a.coordinator.FetchOffset(oreq)
		dbgf("consumer %q received OffsetFetchResponse %v, %v", con.topic, oresp, err)
		if err != nil {
			con.deliverError("fetching offsets", -1, err)
			// and we can't consume any of the new partitions without the offsets
			return
		}

		// start consuming from the added partitions at each partition's last committed offset (which by convention kafaka defines as the last consumed offset+1)
		// since computing the starting offset and beginning to consume requires several round trips to the kafka brokers we start all the
		// partitions concurrently. That reduces the startup time to a couple RTTs even for topics with a numerous partitions.
		started := make(chan *partition)
		var wg sync.WaitGroup
		for _, p := range added {
			wg.Add(1)
			go func(p int32) {
				defer wg.Done()
				ob := oresp.GetBlock(con.topic, p)
				if ob == nil {
					// can't start this partition without an offset
					con.deliverError("FetchOffset response", p, fmt.Errorf("partition %d missing", p))
					return
				}
				if ob.Err != 0 {
					con.deliverError("FetchOffset response", p, ob.Err)
					return
				}

				// run the committed offset through the StartingOffset() hook
				offset, err := con.cl.config.StartingOffset(con.topic, p, ob.Offset, con.cl.client)
				if err != nil {
					con.deliverError("StartingOffset", p, err)
					return
				}

				logf("consumer %q consuming partition %d at offset %d", con.topic, p, offset)

				consumer, err := con.consumer.ConsumePartition(con.topic, p, offset)
				if err != nil {
					con.deliverError(fmt.Sprintf("sarama.ConsumePartition at offset %d", offset), p, err)

					// If the error is ErrOffsetOutOfRange then give ourselves one chance to recover
					if err != sarama.ErrOffsetOutOfRange {
						// otherwise we can't consume this partition.
						return
					}

					offset, err = con.cl.config.OffsetOutOfRange(con.topic, p, con.cl.client)
					if err != nil {
						// should we deliver them their own error? I guess so.
						con.deliverError("OffsetOutOfRange callback", p, err)
						return
					}

					logf("consumer %q skipping to offset %d or partition %d", con.topic, offset, p)
					consumer, err = con.consumer.ConsumePartition(con.topic, p, offset)
					if err != nil {
						con.deliverError(fmt.Sprintf("sarama.ConsumePartition at offset %d", offset), p, err)
						// it didn't work with their offset either. give up
						// (we could go into a loop and call them again, but what would that solve?)
						return
					}
					// it worked with the new offset; carry on
				}

				part := &partition{
					con:       con,
					consumer:  consumer,
					partition: p,
					oldest:    offset,
				}
				go part.run()

				started <- part
			}(p)
		}
		go func() {
			wg.Wait()
			close(started)
		}()

		for part := range started {
			partitions[part.partition] = part
		}
	}

	// restart consuming a partition at a new[er] offset
	restart_partition := func(part *partition) {
		// we kill the old and start a new partition consumer since there is no way to seek an existing sarama.PartitionConsumer in sarama's November 2016 API)
		p := part.partition

		// first remove the old partition consumer. Once it gets a ErrOffsetOutOfRange it's unable to function.
		// since it had an out-of-range offset, it can't commit its offset either
		if pa, ok := partitions[p]; !ok || part != pa {
			// this is an unknown partition, or we've already killed it; ignore the request
			return
		}
		delete(partitions, p)
		part.consumer.Close()

		// then ask what the new starting offset should be
		offset, err := con.cl.config.OffsetOutOfRange(con.topic, p, con.cl.client)
		if err != nil {
			// should we deliver them their own error? I guess so.
			con.deliverError("OffsetOutOfRange callback", p, err)
			return
		}

		// finally make a new partition consuming starting at the given offset
		consumer, err := con.consumer.ConsumePartition(con.topic, p, offset)
		if err != nil {
			con.deliverError(fmt.Sprintf("sarama.ConsumePartition at offset %d", offset), p, err)
			return
		}

		logf("consumer %q restarting consuming partition %d at offset %d", con.topic, p, offset)

		part = &partition{
			con:       con,
			consumer:  consumer,
			partition: p,
			oldest:    offset,
		}
		go part.run()
		partitions[p] = part
	}

	for {
		select {
		case msg := <-con.premessages:
			msgf("premessage msg %q:%d/%d", msg)
			// keep track of msg's offset so we can match it with Done, and deliver the msg
			part := partitions[msg.Partition]
			if part == nil {
				// message from a stale consumer; ignore it
				dbgf("no partition %d", msg.Partition)
				continue
			}
			if part.oldest == sarama.OffsetNewest || part.oldest == sarama.OffsetOldest {
				// we now know the starting offset. make as if we'd been asked to start there
				part.oldest = msg.Offset
			}
			delta := msg.Offset - part.oldest
			if delta < 0 { // || delta > max-out-of-order  (TODO)
				dbgf("stale message %q:%d/%d", msg.Topic, msg.Partition, msg.Offset)
				// we can't take this message into account
				continue
			}
			index := int(delta) >> 6 //  /64
			for index >= len(part.buckets) {
				// add a new bucket
				part.buckets = append(part.buckets, [2]uint8{0, 0})
			}
			part.buckets[index][0]++

			// and deliver the msg (or handle any of the other messages which can arrive)
		deliver_loop:
			for {
				select {
				case con.messages <- msg:
					msgf("delivered msg %q:%d/%d", msg)
					// success
					break deliver_loop

				case msg2 := <-con.done:
					done(msg2)
				case a := <-con.assignments:
					assignment(a)
				case c := <-con.commit_reqs:
					commit_req(c)
				case p := <-con.restart_partitions:
					restart_partition(p)
				case <-con.closed:
					// the defered operations do the work
					return
				}
			}

		case msg := <-con.done:
			done(msg)
		case a := <-con.assignments:
			assignment(a)
		case c := <-con.commit_reqs:
			commit_req(c)
		case p := <-con.restart_partitions:
			restart_partition(p)
		case <-con.closed:
			// the defered operations do the work
			return
		}
	}
}

func (con *consumer) Done(msg *sarama.ConsumerMessage) {
	// send it back to consumer.run to be processed synchronously
	msgf("Done(%q:%d/%d)", msg)
	select {
	case con.done <- msg:
		// great, msg delivered
	case <-con.closed:
		// consumer has closed
	}
}

// partition contains the data associated with us consuming one partition
type partition struct {
	con       *consumer
	consumer  sarama.PartitionConsumer
	partition int32 // partition number
	// buckets of # of offsets read from kafka, and the # of offsets completed by a call to Done(). the difference is the # of offsets in flight in the calling code
	// we group offsets in groups of 64 and simply keep a count of how many are outstanding
	// any time the two counts are equal then the offsets are committable. Otherwise we can't tell which is the not yet Done() offset and so we don't know
	buckets [][2]uint8
	oldest  int64 // 1st offset in bucket[0]
}

// wrap a sarama.ConsumerError into an *Error
func (part *partition) makeConsumerError(cerr *sarama.ConsumerError) *Error {
	Err := part.con.makeError("consuming from sarama", cerr.Err)
	Err.Topic = cerr.Topic
	Err.Partition = cerr.Partition
	return Err
}

// run consumes from the partition and delivers it to the consumer
func (part *partition) run() {
	con := part.con
	defer dbgf("partition consumer of %q partition %d exiting", con.topic, part.partition)
	msgs := part.consumer.Messages()
	errors := part.consumer.Errors()
	for {
		select {
		case msg, ok := <-msgs:
			if ok {
				msgf("got msg %q:%d/%d", msg)
				select {
				case con.premessages <- msg:
				case <-con.closed:
					return
				}
			} else {
				dbgf("draining topic %q partition %d errors", con.topic, part.partition)
				// deliver any remaining errors, and exit
				for sarama_err := range errors {
					con.cl.deliverError("", part.makeConsumerError(sarama_err))
				}
				return
			}
		case sarama_err, ok := <-errors:
			if ok {
				// pick out ErrOffsetOutOfRange errors. These happen if the consumer offset falls off the tail of the kafka log.
				// this easily happens in two cases: when the consumer is too slow, or when the consumer has been stopped for too long.
				// This error cannot be fixed without seeking to a valid offset. However we can't assume that OffsetNewest is the right
				// choice, nor OffsetOldest, nor "5 minutes ago" or anything else. It's up to the user to decide.
				if sarama_err.Err == sarama.ErrOffsetOutOfRange {
					logf("consumer %q partition %d received ErrOffsetOutOfRange and will be restarted", con.topic, part.partition)
					select {
					case con.restart_partitions <- part:
					case <-con.closed:
						return
					}
					// should we keep reading from the partition? it's unlikely to produce much
				}
				// and always deliver the error
				con.cl.deliverError("", part.makeConsumerError(sarama_err))
			} else {
				// finish off any remaining messages, and exit
				dbgf("draining topic %q partition %d msgs", con.topic, part.partition)
				for msg := range msgs {
					select {
					case con.premessages <- msg:
					case <-con.closed:
						return
					}
				}
				return
			}
		}
	}
}

// difference returns the differences (additions and subtractions) between two slices of int32.
// typically the slices contain partition numbers.
func difference(old map[int32]*partition, next []int32) (added, removed []int32) {
	o := make(int32Slice, 0, len(old))
	for p := range old {
		o = append(o, p)
	}

	n := make(int32Slice, len(next))
	copy(n, next)

	sort.Sort(o)
	sort.Sort(n)

	i, j := 0, 0
	for i < len(o) && j < len(n) {
		if o[i] < n[j] {
			removed = append(removed, o[i])
			i++
		} else if o[i] > n[j] {
			added = append(added, n[j])
			j++
		} else {
			i++
			j++
		}
	}
	removed = append(removed, o[i:]...)
	added = append(added, n[j:]...)

	return
}

// a sortable []int32
type int32Slice []int32

func (p int32Slice) Len() int           { return len(p) }
func (p int32Slice) Less(i, j int) bool { return p[i] < p[j] }
func (p int32Slice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }
