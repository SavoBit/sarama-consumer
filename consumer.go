/*
  A simple kafka consumer-group client

  Copyright 2016 MistSys
*/

package consumer

import (
	"fmt"
	"log"
	"sort"
	"sync"
	"time"

	"github.com/Shopify/sarama"
)

const debug = true          // set to true to see debug messages
const per_msg_debug = false // set to true to see per-message debug messages

// dbgf logs a printf style message to somewhere reasonable if debug is enabled, and as efficiently as it can does nothing with any side effects if debug is disabled
func dbgf(fmt string, args ...interface{}) {
	if debug {
		log.Printf(fmt, args...)
	}
}

// msgf is similar to dbgf but used for per-message debug messages. since these are so numerous there's a separate compile-time flag to compile these out
func msgf(fmt string, args ...interface{}) {
	if per_msg_debug {
		log.Printf(fmt, args...)
	}
}

// minimum kafka API version required. Use this when constructing the sarama.Client's sarama.Config.MinVersion
var MinVersion = sarama.V0_9_0_0

// Error holds the errors generated by this package
type Error struct {
	Err       error    // underlying error
	Context   string   // description of the context surrounding the error
	Consumer  Consumer // nil, or Consumer which produced the error
	Topic     string   // "", or the topic which had the error
	Partition int32    // -1, or the partition which had the error
	cl        *client
}

func (err *Error) Error() string {
	if err.Topic != "" {
		if err.Partition != -1 {
			return fmt.Sprintf("consumer-group %q: Error %s, topic %q partition %d: %s", err.cl.group_name, err.Context, err.Topic, err.Partition, err.Err)
		}
		return fmt.Sprintf("consumer-group %q: Error %s, topic %q: %s", err.cl.group_name, err.Context, err.Topic, err.Err)
	}
	return fmt.Sprintf("consumer-group %q: Error %s: %s", err.cl.group_name, err.Context, err.Err)
}

// Config is the configuration of a Client. Typically you'd create a default configuration with
// NewConfig, modify any fields of interest, and pass it to NewClient. Once passed to NewClient the
// Config must not be modified. (doing so leads to data races, and may caused bugs as well).
//
// In addition to this config, consumer's code also looks at the sarama.Config of the sarama.Client
// supplied to NewClient, especially at the Consumer.Offset settings, Version, Metadata.Retry.Backoff
// and [TODO] ChannelBufferSize.
type Config struct {
	Session struct {
		// The allowed session timeout for registered consumers (defaults to 30s).
		// Must be within the allowed server range.
		Timeout time.Duration
	}
	Rebalance struct {
		// The allowed rebalance timeout for registered consumers (defaults to 30s).
		// Must be within the allowed server range. Only functions if sarama.Config.Version >= 0.10.1
		// Otherwise Session.Timeout is used for rebalancing too.
		Timeout time.Duration
	}
	Heartbeat struct {
		// Interval between each heartbeat (defaults to 3s). It should be no more
		// than 1/3rd of the Group.Session.Timout setting
		Interval time.Duration
	}

	// the partitioner used to map partitions to consumer group members (defaults to a round-robin partitioner)
	Partitioner Partitioner

	// The handler for sarama.ErrOffsetOutOfRange errors (defaults to sarama.OffsetNewest,nil). Implementation
	// must return the new starting offset in the partition, or an error. The sarama.Client is included for
	// convenience, since handling this might involve querying the partition's current offsets.
	OffsetOutOfRange func(topic string, partition int32, client sarama.Client) (offset int64, err error)
}

// default implementation of Config.Offsets.OffsetOutOfRange jumps to the current head of the partition.
func DefaultOffsetOutOfRange(topic string, partition int32, client sarama.Client) (offset int64, err error) {
	offset = sarama.OffsetNewest
	return
}

// NewConfig constructs a default configuration.
func NewConfig() *Config {
	cfg := &Config{}
	cfg.Session.Timeout = 30 * time.Second
	cfg.Rebalance.Timeout = 30 * time.Second
	cfg.Heartbeat.Interval = 3 * time.Second
	cfg.Partitioner = RoundRobin
	cfg.OffsetOutOfRange = DefaultOffsetOutOfRange
	return cfg
}

/*
  NewClient creates a new consumer group client on top of an existing
  sarama.Client.

  After this call the contents of config should be treated as read-only.
  config can be nil if the defaults are acceptable.

  The consumer group name is used to match this client with other
  instances running elsewhere, but connected to the same cluster
  of kafka brokers and using the same consumer group name.

  The supplied sarama.Client should have been constructed with a sarama.Config
  where sarama.Config.Version is >= consumer.MinVersion, and if full handling of
  ErrOffsetOutOfRange is desired, sarama.Config.Consumer.Return.Errors = true.

  In addition, this package uses the settings in sarama.Config.Consumer.Offset
*/
func NewClient(group_name string, config *Config, sarama_client sarama.Client) (Client, error) {

	cl := &client{
		client:     sarama_client,
		config:     config,
		group_name: group_name,

		errors: make(chan error),

		closed:       make(chan struct{}),
		add_consumer: make(chan add_consumer),
		rem_consumer: make(chan *consumer),
	}

	// start the client's manager goroutine
	rc := make(chan error)
	cl.wg.Add(1)
	go cl.run(rc)

	return cl, <-rc
}

/*
  Client is a kafaka client belonging to a consumer group. It is created by NewClient.
*/
type Client interface {
	// Consume returns a consumer of the given topic
	Consume(topic string) (Consumer, error)

	// Close closes the client. It must be called to shutdown
	// the client. It calls AsyncClose on any yet unclosed topic
	// Consumers created by this Client.
	// It does NOT close the inner sarama.Client.
	// Calling twice is NOT supported.
	Close()

	// Errors returns a channel which can (should) be monitored
	// for errors. callers should probably log or otherwise report
	// the returned errors. The channel closes when the client
	// is closed.
	Errors() <-chan error

	// TODO have a Status() method for debug/logging? Or is Errors() enough?
}

/*
  Consumer is a consumer of a topic.

  Messages from any partition assigned to this client arrive on the
  channel returned by Messages.

  Every message read from the Messages channel must be eventually passed
  to Done. Calling Done is the signal that that message has been consumed
  and the offset of that message can be committed back to kafka.

  Of course this requires that the message's Partition and Offset fields not
  be altered. Then again for what possible reason would you do such a thing?
*/
type Consumer interface {
	// Messages returns the channel of messages arriving from kafka. It always
	// returns the same result, so it is safe to call once and store the result.
	// Every message read from the channel should be passed to Done when processing
	// of the message is complete.
	Messages() <-chan *sarama.ConsumerMessage

	// Done indicates the processing of the message is complete, and its offset can
	// be committed to kafka. Calling Done twice with the same message, or with a
	// garbage message, can cause trouble.
	Done(*sarama.ConsumerMessage)

	// AsyncClose terminates the consumer cleanly. Callers can continue to read from
	// Messages channel until it is closed, or not, as they wish.
	// Calling Client.Close() performs a AsyncClose() on any remaining consumers.
	// Calling AsyncClose multiple times is permitted. Only the first call has any effect.
	// Never calling AsyncClose is also permitted. Client.Close() implies Consumer.AsyncClose.
	AsyncClose()

	// Close() terminates the consumer and waits for it to be finished comitting the current
	// offsets to kafka.
	Close()
}

/*
  Partitioner maps partitions to consumer group members.

  When the user wants control over the partitioning they should set
  Config.Partitioner to their implementation of Partitioner.
*/
type Partitioner interface {
	// PrepareJoin prepares a JoinGroupRequest given the topics supplied.
	// The simplest implementation would be something like
	//   join_req.AddGroupProtocolMetadata("<partitioner name>", &sarama.ConsumerGroupMemberMetadata{ Version: 1, Topics:  topics, })
	PrepareJoin(join_req *sarama.JoinGroupRequest, topics []string)

	// Partition performs the partitioning. Given the requested
	// memberships from the JoinGroupResponse, it adds the results
	// to the SyncGroupRequest. Returning an error cancels everything.
	// The sarama.Client supplied to NewClient is included for convenince,
	// since performing the partitioning probably requires looking at each
	// topic's metadata, especially its list of partitions.
	Partition(*sarama.SyncGroupRequest, *sarama.JoinGroupResponse, sarama.Client) error

	// ParseSync parses the SyncGroupResponse and returns the map of topics
	// to partitions assigned to this client, or an error if the information
	// is not parsable.
	ParseSync(*sarama.SyncGroupResponse) (map[string][]int32, error)
}

// client implements the Client interface
type client struct {
	client     sarama.Client // the sarama client we were constructed from
	config     *Config       // our configuration (read-only)
	group_name string        // the client-group name

	errors chan error // channel over which asynchronous errors are reported

	closed chan struct{}  // channel which is closed to cause the client to shutdown
	wg     sync.WaitGroup // waitgroup which is done when the client is shutdown

	add_consumer chan add_consumer // command channel used to add a new consumer
	rem_consumer chan *consumer    // command channel used to remove an existing consumer
}

// Errors returns the channel over which asynchronous errors are observed.
func (cl *client) Errors() <-chan error { return cl.errors }

// add_consumer are the messages sent over the client.add_consumer channel
type add_consumer struct {
	con   *consumer
	reply chan<- error
}

func (cl *client) Consume(topic string) (Consumer, error) {
	sarama_consumer, err := sarama.NewConsumerFromClient(cl.client)
	if err != nil {
		return nil, cl.makeError("Consume sarama.NewConsumerFromClient", err)
	}

	con := &consumer{
		cl:       cl,
		consumer: sarama_consumer,
		topic:    topic,

		messages: make(chan *sarama.ConsumerMessage),

		closed: make(chan struct{}),
		exited: make(chan struct{}),

		assignments: make(chan *assignment, 1),
		commit_reqs: make(chan commit_req),

		restart_partitions: make(chan *partition),
		premessages:        make(chan *sarama.ConsumerMessage), // TODO give ourselves some capacity once I know it runs right without any (capacity hides bugs :-)
		done:               make(chan *sarama.ConsumerMessage), // we should probably use sarama.Config.ChannelBufferSize for our channels
	}

	reply := make(chan error)
	cl.add_consumer <- add_consumer{con, reply}
	err = <-reply
	if err != nil {
		// if an error is returned then it is up to us to close the sarama.Consumer
		_ = sarama_consumer.Close() // we already have an error to return. a 2nd one is too much
		return nil, err
	}
	return con, nil
}

// Close shutsdown the client and any remaining Consumers.
func (cl *client) Close() {
	// signal to cl.run() that it should exit
	dbgf("Close client of consumer-group %q", cl.group_name)
	close(cl.closed)
	// and wait for the shutdown to be complete
	cl.wg.Wait()
}

// run is a long lived goroutine which manages this client's membership in the consumer group.
func (cl *client) run(early_rc chan<- error) {
	defer cl.wg.Done()

	var member_id string                    // our group member id, assigned to us by kafka when we first make contact
	consumers := make(map[string]*consumer) // map of topic -> consumer
	var wg sync.WaitGroup                   // waitgroup used to wait for all consumers to exit

	defer dbgf("consumer-group %q client exiting", cl.group_name)

	// add a consumer
	add := func(add add_consumer) {
		dbgf("client.run add(topic %q)", add.con.topic)
		if _, ok := consumers[add.con.topic]; ok {
			// topic already is being consumed. the way the standard kafka 0.9 group coordination works you cannot consume twice with the
			// same client. If you want to consume the same topic twice, use two Clients.
			add.reply <- cl.makeError("Consume", fmt.Errorf("topic %q is already being consumed", add.con.topic))
			return
		}
		consumers[add.con.topic] = add.con
		wg.Add(1)
		go add.con.run(&wg)
		add.reply <- nil
	}
	// remove a consumer
	rem := func(con *consumer) {
		dbgf("client.run rem(topic %q)", con.topic)
		existing_con := consumers[con.topic]
		if existing_con == con {
			delete(consumers, con.topic)
		} // else it's some old consumer and we've already removed it
		// and let the consumer shutdown
		close(con.assignments)
		close(con.commit_reqs)
	}
	// shutdown the consumers. waits until they are all stopped. only call once and return afterwards, since it makes assumptions that hold only when it is used like that
	shutdown := func() {
		dbgf("client.run shutdown")
		// shutdown the remaining consumers
		for _, con := range consumers {
			con.AsyncClose()
		}
		// and consume any last rem_consumer messages from them
		go func() {
			wg.Wait()
			close(cl.rem_consumer)
		}()
		for con := range cl.rem_consumer {
			rem(con)
		}
		// and shutdown the errors channel
		close(cl.errors)
	}

	pause := false
	refresh := false        // refresh the coordinating broker (after an I/O error or a ErrNotCoordinatorForConsumer)
	reopen := false         // reopen coordinating broker (after an I/O error)
	var coor *sarama.Broker // nil, or coordinating broker

	// loop rejoining the group each time the group reforms
join_loop:
	for {
		if pause {
			delay := cl.client.Config().Metadata.Retry.Backoff // TODO should we increase timeouts?
			dbgf("pausing %v", delay)
			// pause before continuing, so we don't fail continuously too fast
			timeout := time.After(delay)
		pause_loop:
			for {
				select {
				case <-timeout:
					break pause_loop
				case <-cl.closed:
					// shutdown the remaining consumers
					shutdown()
					return
				case a := <-cl.add_consumer:
					add(a)
				case r := <-cl.rem_consumer:
					rem(r)
				}
			}
			pause = false
		}

		if reopen {
			if coor != nil {
				dbgf("closing and reopening connection to coordinator %d %s", coor.ID(), coor.Addr())
				if ok, err := coor.Connected(); ok {
					err = coor.Close()
					if err != nil {
						cl.deliverError(fmt.Sprintf("Close()ing coordinating broker %d %s", coor.ID(), coor.Addr()), err)
					}
				}
				err := coor.Open(cl.client.Config())
				if err != nil {
					cl.deliverError(fmt.Sprintf("re Open()ing coordinating broker %d %s", coor.ID(), coor.Addr()), err)
				}
			}
			reopen = false

			// after reopening (successfully or not), always refresh the coordinator
			refresh = true
		}

		if refresh {
			dbgf("refreshing coordinating broker")

			// refresh the group coordinator (because sarama caches the result, and the cache must be manually invalidated by us when we decide it might be needed)
			err := cl.client.RefreshCoordinator(cl.group_name)
			if err != nil {
				err = cl.makeError("refreshing coordinating broker", err)
				if early_rc != nil {
					early_rc <- err
					return
				}
				cl.deliverError("", err)
				pause = true
				continue join_loop
			}
			refresh = false
		}

		// make contact with the kafka broker coordinating this group
		// NOTE: sarama keeps the result cached, so we aren't taking a round trip to the kafka brokers very time
		// (then again we need to manage sarama's cache too)
		var err error
		coor, err = cl.client.Coordinator(cl.group_name)
		if err != nil {
			err = cl.makeError("contacting coordinating broker", err)
			if early_rc != nil {
				early_rc <- err
				return
			}
			cl.deliverError("", err)

			pause = true
			refresh = true
			continue join_loop
		}
		dbgf("Coordinator %v", coor)

		// join the group
		jreq := &sarama.JoinGroupRequest{
			GroupId:        cl.group_name,
			SessionTimeout: int32(cl.config.Session.Timeout / time.Millisecond),
			MemberId:       member_id,
			ProtocolType:   "consumer", // we implement the standard kafka 0.9 consumer protocol metadata
		}

		var topics = make([]string, 0, len(consumers))
		for topic := range consumers {
			topics = append(topics, topic)
		}
		cl.config.Partitioner.PrepareJoin(jreq, topics)

		dbgf("sending JoinGroupRequest %v", jreq)
		jresp, err := coor.JoinGroup(jreq)
		dbgf("received JoinGroupResponse %v, %v", jresp, err)
		if err != nil {
			// some I/O error happened; we should reopen and refresh the current coordinator
			reopen = true
		} else if jresp.Err != 0 {
			switch jresp.Err {
			case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
				refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
			case sarama.ErrUnknownMemberId:
				member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
			}
			err = jresp.Err
		}
		if err != nil {
			err = cl.makeError("joining group", err)
			// if it is still early (the 1st iteration of this loop) then return the error and bail out
			if early_rc != nil {
				early_rc <- err
				return
			}
			cl.deliverError("", err)

			pause = true
			continue join_loop
		}

		// we managed to get a successfull join-group response. that is far enough that basic communication is functioning
		// and we can declare that our early_rc is success and release the caller to NewClient
		if early_rc != nil {
			early_rc <- nil
			early_rc = nil
		}

		// save our member_id for next time we join, and the new generation id
		member_id = jresp.MemberId
		generation_id := jresp.GenerationId
		dbgf("member_id %q, generation_id %d", member_id, generation_id)

		// prepare a sync request
		sreq := &sarama.SyncGroupRequest{
			GroupId:      cl.group_name,
			GenerationId: generation_id,
			MemberId:     member_id,
		}

		// we have been chosen as the leader then we have to map the partitions
		if jresp.LeaderId == member_id {
			dbgf("leader is we")
			err := cl.config.Partitioner.Partition(sreq, jresp, cl.client)
			if err != nil {
				cl.deliverError("partitioning", err)
				// and rejoin (thus aborting this generation) since we can't partition it as needed
				pause = true
				continue join_loop
			}
		}

		// send SyncGroup
		dbgf("sending SyncGroupRequest %v", sreq)
		sresp, err := coor.SyncGroup(sreq)
		dbgf("received SyncGroupResponse %v, %v", sresp, err)
		if err != nil {
			reopen = true
		} else if sresp.Err != 0 {
			switch sresp.Err {
			case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
				refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
			case sarama.ErrUnknownMemberId:
				member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
			}
			err = sresp.Err
		}
		if err != nil {
			cl.deliverError("synchronizing group", err)
			pause = true
			continue join_loop
		}
		assignments, err := cl.config.Partitioner.ParseSync(sresp)
		if err != nil {
			cl.deliverError("decoding member assignments", err)
			pause = true
			continue join_loop
		}

		// save and distribute the new assignments to our topic consumers
		a := &assignment{
			generation_id: generation_id,
			coordinator:   coor,
			member_id:     member_id,
			assignments:   assignments,
		}
		for _, con := range consumers {
			select {
			case con.assignments <- a:
				// got it on the first try
			default:
				// con.assignment is full (it has a capacity of 1)
				// remove the stale assignment and place this one in its place
				select {
				case <-con.assignments:
					// we have room now (since we're the only code which writes to this channel)
					con.assignments <- a
				case con.assignments <- a:
					// in this case the consumer removed the stale assignment before we could
				}
			}
		}

		// start the heartbeat timer
		heartbeat_timer := time.After(cl.config.Heartbeat.Interval)
		// and the offset commit timer
		clconfig := cl.client.Config()
		commit_timer := time.After(clconfig.Consumer.Offsets.CommitInterval)

		// and loop, sending heartbeats until something happens and we need to rejoin (or exit)
		for {
			select {
			case <-cl.closed:
				// cl.Close() has been called; time to exit

				// shutdown any remaining consumers (causing them to sync their final offsets)
				shutdown()

				// and nicely leave the consumer group
				req := &sarama.LeaveGroupRequest{
					GroupId:  cl.group_name,
					MemberId: member_id,
				}
				dbgf("sending LeaveGroupRequest %v", req)
				resp, err := coor.LeaveGroup(req)
				dbgf("received LeaveGroupResponse %v, %v", resp, err)
				// note: we don't bother with the full error handling code, since we're exiting anyway
				if err == nil && resp.Err != 0 {
					err = resp.Err
				}
				if err != nil {
					cl.deliverError("leaving group", err)
				}

				// and we're done
				return

			case <-heartbeat_timer:
				// send a heartbeat
				req := &sarama.HeartbeatRequest{
					GroupId:      cl.group_name,
					MemberId:     member_id,
					GenerationId: generation_id,
				}
				dbgf("sending HeartbeatRequest %v", req)
				resp, err := coor.Heartbeat(req)
				dbgf("received HeartbeatResponse %v, %v", resp, err)
				if err != nil {
					reopen = true
				} else if resp.Err != 0 {
					switch resp.Err {
					case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
						refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
					case sarama.ErrUnknownMemberId:
						member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
					}
					err = resp.Err
				}
				if err != nil {
					cl.deliverError("heartbeating", err)
					// we've got heartbeat troubles of one kind or another; disconnect and reconnect
					continue join_loop
				}

				// and start the next heartbeat only after we get the response to this one
				// that way when the network or the broker are slow we back off.
				heartbeat_timer = time.After(cl.config.Heartbeat.Interval)

			case <-commit_timer:
				ocreq := &sarama.OffsetCommitRequest{
					ConsumerGroup:           cl.group_name,
					ConsumerGroupGeneration: generation_id,
					ConsumerID:              member_id,
					RetentionTime:           int64(clconfig.Consumer.Offsets.Retention / time.Millisecond),
					Version:                 2, // kafka 0.9.0 version, with RetentionTime
				}
				if clconfig.Consumer.Offsets.Retention == 0 { // note that this and the rounding math above means that if you wanted a retention time of 0 millseconds you could set Config.Offsets.RetentionTime to something < 1 ms, like 1 nanosecond
					ocreq.RetentionTime = -1 // use broker's value
				}
				var wg sync.WaitGroup
				for _, con := range consumers {
					// NOTE we must wait for each consumer to finish adding itself before sending the commit_req to the next consumer
					// otherwise they race on calling ocreq.AddBlock() and will cause concurrent hashmap writes.
					wg.Add(1)
					con.commit_reqs <- commit_req{ocreq, &wg}
					wg.Wait()
				}
				dbgf("sending OffsetCommitRequest %v", ocreq)
				ocresp, err := coor.CommitOffset(ocreq)
				dbgf("received OffsetCommitResponse %v, %v", ocresp, err)
				// log any errors we got. there isn't much we can do about them
				if err != nil {
					cl.deliverError("committing offsets", err)
					reopen = true
				} else {
					for topic, partitions := range ocresp.Errors {
						for p, kerr := range partitions {
							if kerr != 0 {
								cl.deliverError(fmt.Sprintf("committing offset of topic %q partition %d", topic, p), kerr)
								switch kerr {
								case sarama.ErrNotCoordinatorForConsumer, sarama.ErrConsumerCoordinatorNotAvailable, sarama.ErrRebalanceInProgress:
									refresh = true // the broker is no longer the coordinator. we should refresh the current coordinator
								case sarama.ErrUnknownMemberId:
									member_id = "" // the coordinator no longer knows who we are; have it assign us a new member id
								}
								err = kerr // any of the kerr's will do
							}
						}
					}
				}
				if err != nil {
					continue join_loop
				}

				commit_timer = time.After(clconfig.Consumer.Offsets.CommitInterval)

			case a := <-cl.add_consumer:
				add(a)
				// and rejoin so we can become a member of the new topic
				continue join_loop
			case r := <-cl.rem_consumer:
				rem(r)
				// and rejoin so we can be removed as member of the new topic
				continue join_loop
			}
		} // end of heartbeat loop
	} // end of join_loop
}

// makeError wraps err into a *Error, associating it with context
func (cl *client) makeError(context string, err error) *Error {
	return &Error{
		cl:        cl,
		Err:       err,
		Context:   context,
		Topic:     "",
		Partition: -1,
	}
}

// deliverError builds an error and delivers it to the channel returned by cl.Errors
func (cl *client) deliverError(context string, err error) {
	if context != "" {
		err = cl.makeError(context, err)
	}
	dbgf("%v", err)
	cl.errors <- err
}

// consumer implements the Consumer interface
type consumer struct {
	cl       *client
	consumer sarama.Consumer
	topic    string

	messages chan *sarama.ConsumerMessage

	closed     chan struct{} // channel which is closed when the consumer is AsyncClose()ed
	close_once sync.Once     // Once used to make sure we close only once
	exited     chan struct{} // channel which is closed when the consumer is far enough along in exiting that consumer.Close can return

	assignments chan *assignment // channel over which client.run sends consumer.run each generation's partition assignments
	commit_reqs chan commit_req  // channel over which client.run sends consumer.run request to fill out a OffsetCommitRequest

	restart_partitions chan *partition              // channel through which partition.run delivers partition restart [at new offset] requests
	premessages        chan *sarama.ConsumerMessage // channel through which partition.run delivers messages to consumer.run
	done               chan *sarama.ConsumerMessage // channel through which Done() returns messages
}

// commit_req is a request for a consumer to write its part into a OffsetCommitRequest
type commit_req struct {
	ocreq *sarama.OffsetCommitRequest
	wg    *sync.WaitGroup
}

// assignment is this client's assigned partitions
type assignment struct {
	generation_id int32              // the current generation
	coordinator   *sarama.Broker     // the current client-group coordinating broker
	member_id     string             // the member_id assigned to us by the coordinator
	assignments   map[string][]int32 // map of topic -> list of partitions
}

// construct a *Error from this consumer
func (con *consumer) makeError(context string, err error) *Error {
	Err := con.cl.makeError(context, err)
	Err.Consumer = con
	Err.Topic = con.topic
	return Err
}

// construct and deliver an *Error from this consumer
func (con *consumer) deliverError(context string, partition int32, err error) {
	Err := con.makeError(context, err)
	Err.Partition = partition
	con.cl.deliverError("", Err)
}

func (con *consumer) Messages() <-chan *sarama.ConsumerMessage { return con.messages }

// close the consumer. it can safely be called multiple times
func (con *consumer) AsyncClose() {
	dbgf("AsyncClose consumer of topic %q", con.topic)
	con.close_once.Do(func() { close(con.closed) })
}

// close the consumer and wait
func (con *consumer) Close() {
	dbgf("Close consumer of topic %q", con.topic)
	con.AsyncClose() // initiate the shutdown
	<-con.exited     // and wait around until it is complete
}

// consumer goroutine coordinates consuming from multiple partitions in a topic
// NOTE WELL: this function must never do anything which would prevent it from processing message from client.run promptly.
// That means any channel I/O must include cases for con.assignments and con.commit_reqs.
func (con *consumer) run(wg *sync.WaitGroup) {
	var generation_id int32 // current generation
	var coor *sarama.Broker // current consumer group coordinating broker
	var member_id string    // our member id assigned by coor

	partitions := make(map[int32]*partition) // map of partition number -> partition consumer

	// shutdown the removed partitions, committing their last offset
	remove := func(removed []int32) {
		dbgf("consumer %q rem(%v)", con.topic, removed)
		if len(removed) == 0 {
			// nothing to do, and no point in sending an empty OffsetCommitRequest msg either
			return
		}
		clconfig := con.cl.client.Config()
		ocreq := &sarama.OffsetCommitRequest{
			ConsumerGroup:           con.cl.group_name,
			ConsumerGroupGeneration: generation_id,
			ConsumerID:              member_id,
			RetentionTime:           int64(clconfig.Consumer.Offsets.Retention / time.Millisecond),
			Version:                 2, // kafka 0.9.0 version, with RetentionTime
		}
		if clconfig.Consumer.Offsets.Retention == 0 { // note that this and the rounding math above means that if you wanted a retention time of 0 millseconds you could set Config.Offsets.RetentionTime to something < 1 ms, like 1 nanosecond
			ocreq.RetentionTime = -1 // use broker's value
		}
		for _, p := range removed {
			// stop consuming from partition p
			if part, ok := partitions[p]; ok {
				delete(partitions, p)
				part.consumer.Close()
				offset := part.oldest
				if len(part.buckets) != 0 {
					if part.buckets[0][0] == part.buckets[0][1] {
						// add to that the portion of the last block we know been completed (this is often useful when the traffic rate is low or a client shuts down cleanly, since it has probably cleanly returned all offsets we've delivered)
						offset += int64(part.buckets[0][1])
					} // else we don't know enough to commit any further
				}
				dbgf("ocreq.AddBlock(%q, %d, %d)", con.topic, p, offset)
				ocreq.AddBlock(con.topic, p, offset, 0, "")
			}
		}
		dbgf("sending OffsetCommitRequest %v", ocreq)
		ocresp, err := coor.CommitOffset(ocreq)
		dbgf("received OffsetCommitResponse %v, %v", ocresp, err)
		// log any errors we got. there isn't much we can do about them; the next consumer will start at an older offset
		if err != nil {
			con.deliverError("committing offsets", -1, err)
		} else {
			for _, partitions := range ocresp.Errors {
				for p, err := range partitions {
					if err != 0 {
						con.deliverError("committing offset", p, err)
					}
				}
			}
		}
	}

	// handle a commit request from client.run
	commit_req := func(c commit_req) {
		dbgf("consumer %q commit_req(%v)", con.topic, c)
		for p, partition := range partitions {
			offset := partition.oldest
			if len(partition.buckets) != 0 {
				if partition.buckets[0][0] == partition.buckets[0][1] {
					// add to that the portion of the last block we know been completed (this is useful when the message rate is slow)
					offset += int64(partition.buckets[0][1])
				} // else we don't know enough to commit any further
			}
			dbgf("ocreq.AddBlock(%q, %d, %d)", con.topic, p, offset)
			c.ocreq.AddBlock(con.topic, p, offset, 0, "")
		}
		c.wg.Done()
	}

	defer func() {
		if len(partitions) != 0 {
			// cleanup the remaining partition consumers
			removed := make([]int32, 0, len(partitions))
			for p := range partitions {
				removed = append(removed, p)
			}
			remove(removed)
		}

		con.consumer.Close()
		close(con.messages)

		// send ourselves to rem_consumer
	rem_loop:
		for {
			select {
			case c := <-con.commit_reqs:
				commit_req(c)
			case <-con.assignments:
				// ignore them, we're shutting down
			case con.cl.rem_consumer <- con:
				break rem_loop
			}
			// NOTE: <-con.done is not a case above because there is no good way to shut it down. it is never closed,
			// so we'd never know when it was drained. As a consequence, it's client.Done() which aborts when the
			// consumer closes, rather than us draining con.done
		}

		// drain any remaining requests from run.client, until
		// run.client closes the channels
		for c := range con.commit_reqs {
			commit_req(c)
		}
		for range con.assignments {
			// ignore them
		}

		dbgf("consumer of topic %q exiting", con.topic)
		close(con.exited)
		wg.Done()
	}()

	// handle a message sent to us via con.done
	done := func(msg *sarama.ConsumerMessage) {
		msgf("consumer %q done(%q:%d/%d)", con.topic, msg.Topic, msg.Partition, msg.Offset)

		// a sanity check, just in case someone passes the msg into the wrong consumer
		if con.topic != msg.Topic {
			con.deliverError("Done()", -1, fmt.Errorf("BUG: Message from topic %q passed to consumer(%q).Done()", msg.Topic, con.topic))
			return
		}

		part := partitions[msg.Partition]
		if part == nil {
			dbgf("no partition %d in topic %q", msg.Partition, con.topic)
			return
		}
		delta := msg.Offset - part.oldest
		if delta < 0 {
			dbgf("stale message %q:%d/%d", msg.Topic, msg.Partition, msg.Offset)
			return
		}
		index := int(delta) >> 6 //  /64
		if index >= len(part.buckets) {
			dbgf("early message %d/%d", msg.Partition, msg.Offset)
			return
		}
		part.buckets[index][1]++
		if index == 0 {
			// we might have finished the oldest bucket
			for part.buckets[0] == [2]uint8{64, 64} {
				// the oldest bucket is complete; advance the last committed offset
				part.oldest += 64
				part.buckets = part.buckets[1:]
				if len(part.buckets) == 0 {
					break
				}
			}
		}
	}

	// handle an assignment message
	assignment := func(a *assignment) {
		dbgf("consumer %q assignment(%v)", con.topic, a)
		// see what has changed in the partition assignment of our topic
		new_partitions := a.assignments[con.topic]
		added, removed := difference(partitions, new_partitions)
		dbgf("consumer %q added %v, removed %v", con.topic, added, removed)

		// shutdown the partitions while we still belong to the previous generation
		remove(removed)

		// update the current generation and related info after committing the last offsets from the previous generation
		generation_id = a.generation_id
		coor = a.coordinator
		member_id = a.member_id

		if len(added) == 0 {
			// we're done early
			return
		}

		// the sarama-cluster code pauses here so that other consumers have time to sync their offsets. Should we do the same?
		// I've observed with kafka 0.9.0.1 that once the coordinator bumps the generation_id the client can't commit an offset with
		// the old id. So unless the client lies and sends generation_id+1 when it commits there is nothing it can commit, and there
		// is no point in waiting. So for now, no waiting.

		// fetch the last committed offsets of the new partitions
		oreq := &sarama.OffsetFetchRequest{
			ConsumerGroup: con.cl.group_name,
			Version:       1, // kafka 0.9.0 expects version 1 offset requests
		}
		for _, p := range added {
			oreq.AddPartition(con.topic, p)
		}

		dbgf("consumer %q sending OffsetFetchRequest %v", con.topic, oreq)
		oresp, err := a.coordinator.FetchOffset(oreq)
		dbgf("consumer %q received OffsetFetchResponse %v, %v", con.topic, oresp, err)
		if err != nil {
			con.deliverError("fetching offsets", -1, err)
			// and we can't consume any of the new partitions without the offsets
			return
		}

		// start consuming from the added partitions at each partition's last committed offset (which by convention kafaka defines as the last consumed offset+1)
		// since computing the starting offset and beginning to consume requires several round trips to the kafka brokers we start all the
		// partitions concurrently. That reduces the startup time to a couple RTTs even for topics with a numerous partitions.
		started := make(chan *partition)
		var wg sync.WaitGroup
		for _, p := range added {
			wg.Add(1)
			go func(p int32) {
				defer wg.Done()
				ob := oresp.GetBlock(con.topic, p)
				if ob == nil {
					// can't start this partition without an offset
					con.deliverError("FetchOffset response", p, fmt.Errorf("partition %d missing", p))
					return
				}
				if ob.Err != 0 {
					con.deliverError("FetchOffset response", p, ob.Err)
					return
				}

				offset := ob.Offset
				if offset == sarama.OffsetNewest {
					// the broker doesn't have an offset for is. Use the configured initial offset
					offset = con.cl.client.Config().Consumer.Offsets.Initial
				}

				dbgf("consumer %q consuming partition %d at offset %d", con.topic, p, offset)

				consumer, err := con.consumer.ConsumePartition(con.topic, p, offset)
				if err != nil {
					con.deliverError(fmt.Sprintf("sarama.ConsumePartition at offset %d", offset), p, err)

					// If the error is ErrOffsetOutOfRange then give ourselves one chance to recover
					if err != sarama.ErrOffsetOutOfRange {
						// otherwise we can't consume this partition.
						return
					}

					offset, err = con.cl.config.OffsetOutOfRange(con.topic, p, con.cl.client)
					if err != nil {
						// should we deliver them their own error? I guess so.
						con.deliverError("OffsetOutOfRange callback", p, err)
						return
					}

					consumer, err = con.consumer.ConsumePartition(con.topic, p, offset)
					if err != nil {
						con.deliverError(fmt.Sprintf("sarama.ConsumePartition at offset %d", offset), p, err)
						// it didn't work with their offset either. give up
						// (we could go into a loop and call them again, but what would that solve?)
						return
					}
					// it worked with the new offset; carry on
				}

				part := &partition{
					con:       con,
					consumer:  consumer,
					partition: p,
					oldest:    offset,
				}
				go part.run()

				started <- part
			}(p)
		}
		go func() {
			wg.Wait()
			close(started)
		}()

		for part := range started {
			partitions[part.partition] = part
		}
	}

	// restart consuming a partition at a new[er] offset
	restart_partition := func(part *partition) {
		// we kill the old and start a new partition consumer since there is no way to seek an existing sarama.PartitionConsumer in sarama's November 2016 API)
		p := part.partition

		// first remove the old partition consumer. Once it gets a ErrOffsetOutOfRange it's unable to function.
		// since it had an out-of-range offset, it can't commit its offset either
		if pa, ok := partitions[p]; !ok || part != pa {
			// this is an unknown partition, or we've already killed it; ignore the request
			return
		}
		delete(partitions, p)
		part.consumer.Close()

		// then ask what the new starting offset should be
		offset, err := con.cl.config.OffsetOutOfRange(con.topic, p, con.cl.client)
		if err != nil {
			// should we deliver them their own error? I guess so.
			con.deliverError("OffsetOutOfRange callback", p, err)
			return
		}

		// finally make a new partition consuming starting at the given offset
		consumer, err := con.consumer.ConsumePartition(con.topic, p, offset)
		if err != nil {
			con.deliverError(fmt.Sprintf("sarama.ConsumePartition at offset %d", offset), p, err)
			return
		}

		part = &partition{
			con:       con,
			consumer:  consumer,
			partition: p,
			oldest:    offset,
		}
		go part.run()
		partitions[p] = part
	}

	for {
		select {
		case msg := <-con.premessages:
			msgf("premessage msg %q:%d/%d", msg.Topic, msg.Partition, msg.Offset)
			// keep track of msg's offset so we can match it with Done, and deliver the msg
			part := partitions[msg.Partition]
			if part == nil {
				// message from a stale consumer; ignore it
				dbgf("no partition %d", msg.Partition)
				continue
			}
			if part.oldest == sarama.OffsetNewest || part.oldest == sarama.OffsetOldest {
				// we now know the starting offset. make as if we'd been asked to start there
				part.oldest = msg.Offset
			}
			delta := msg.Offset - part.oldest
			if delta < 0 { // || delta > max-out-of-order  (TODO)
				dbgf("stale message %q:%d/%d", msg.Topic, msg.Partition, msg.Offset)
				// we can't take this message into account
				continue
			}
			index := int(delta) >> 6 //  /64
			for index >= len(part.buckets) {
				// add a new bucket
				part.buckets = append(part.buckets, [2]uint8{0, 0})
			}
			part.buckets[index][0]++

			// and deliver the msg (or handle any of the other messages which can arrive)
		deliver_loop:
			for {
				select {
				case con.messages <- msg:
					msgf("delivered msg %q:%d/%d", msg.Topic, msg.Partition, msg.Offset)
					// success
					break deliver_loop

				case msg2 := <-con.done:
					done(msg2)
				case a := <-con.assignments:
					assignment(a)
				case c := <-con.commit_reqs:
					commit_req(c)
				case p := <-con.restart_partitions:
					restart_partition(p)
				case <-con.closed:
					// the defered operations do the work
					return
				}
			}

		case msg := <-con.done:
			done(msg)
		case a := <-con.assignments:
			assignment(a)
		case c := <-con.commit_reqs:
			commit_req(c)
		case p := <-con.restart_partitions:
			restart_partition(p)
		case <-con.closed:
			// the defered operations do the work
			return
		}
	}
}

func (con *consumer) Done(msg *sarama.ConsumerMessage) {
	// send it back to consumer.run to be processed synchronously
	msgf("Done(%q:%d/%d)", msg.Topic, msg.Partition, msg.Offset)
	select {
	case con.done <- msg:
		// great, msg delivered
	case <-con.closed:
		// consumer has closed
	}
}

// partition contains the data associated with us consuming one partition
type partition struct {
	con       *consumer
	consumer  sarama.PartitionConsumer
	partition int32 // partition number
	// buckets of # of offsets read from kafka, and the # of offsets completed by a call to Done(). the difference is the # of offsets in flight in the calling code
	// we group offsets in groups of 64 and simply keep a count of how many are outstanding
	// any time the two counts are equal then the offsets are committable. Otherwise we can't tell which is the not yet Done() offset and so we don't know
	buckets [][2]uint8
	oldest  int64 // 1st offset in bucket[0]
}

// wrap a sarama.ConsumerError into an *Error
func (part *partition) makeConsumerError(cerr *sarama.ConsumerError) *Error {
	Err := part.con.makeError("consuming from sarama", cerr.Err)
	Err.Topic = cerr.Topic
	Err.Partition = cerr.Partition
	return Err
}

// run consumes from the partition and delivers it to the consumer
func (part *partition) run() {
	con := part.con
	defer dbgf("partition consumer of %q partition %d exiting", con.topic, part.partition)
	msgs := part.consumer.Messages()
	errors := part.consumer.Errors()
	for {
		select {
		case msg, ok := <-msgs:
			if ok {
				msgf("got msg %q:%d/%d", msg.Topic, msg.Partition, msg.Offset)
				select {
				case con.premessages <- msg:
				case <-con.closed:
					return
				}
			} else {
				dbgf("draining topic %q partition %d errors", con.topic, part.partition)
				// deliver any remaining errors, and exit
				for sarama_err := range errors {
					con.cl.deliverError("", part.makeConsumerError(sarama_err))
				}
				return
			}
		case sarama_err, ok := <-errors:
			if ok {
				// pick out ErrOffsetOutOfRange errors. These happen if the consumer offset falls off the tail of the kafka log.
				// this easily happens in two cases: when the consumer is too slow, or when the consumer has been stopped for too long.
				// This error cannot be fixed without seeking to a valid offset. However we can't assume that OffsetNewest is the right
				// choice, nor OffsetOldest, nor "5 minutes ago" or anything else. It's up to the user to decide.
				if sarama_err.Err == sarama.ErrOffsetOutOfRange {
					dbgf("ErrOffsetOutOfRange topic %q partition %d", con.topic, part.partition)
					select {
					case con.restart_partitions <- part:
					case <-con.closed:
						return
					}
					// should we keep reading from the partition? it's unlikely to produce much
				}
				// and always deliver the error
				con.cl.deliverError("", part.makeConsumerError(sarama_err))
			} else {
				// finish off any remaining messages, and exit
				dbgf("draining topic %q partition %d msgs", con.topic, part.partition)
				for msg := range msgs {
					select {
					case con.premessages <- msg:
					case <-con.closed:
						return
					}
				}
				return
			}
		}
	}
}

// difference returns the differences (additions and subtractions) between two slices of int32.
// typically the slices contain partition numbers.
func difference(old map[int32]*partition, next []int32) (added, removed []int32) {
	o := make(int32Slice, 0, len(old))
	for p := range old {
		o = append(o, p)
	}

	n := make(int32Slice, len(next))
	copy(n, next)

	sort.Sort(o)
	sort.Sort(n)

	i, j := 0, 0
	for i < len(o) && j < len(n) {
		if o[i] < n[j] {
			removed = append(removed, o[i])
			i++
		} else if o[i] > n[j] {
			added = append(added, n[j])
			j++
		} else {
			i++
			j++
		}
	}
	removed = append(removed, o[i:]...)
	added = append(added, n[j:]...)

	return
}

// a sortable []int32
type int32Slice []int32

func (p int32Slice) Len() int           { return len(p) }
func (p int32Slice) Less(i, j int) bool { return p[i] < p[j] }
func (p int32Slice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }

// a simple partitioner that assigns partitions round-robin across all consumers requesting each topic
type roundRobinPartitioner string

// global instance of the round-robin partitioner
const RoundRobin roundRobinPartitioner = "roundrobin" // use the string "roundrobin" without a dash to match what kafka java code uses, should someone want to mix go and java consumers in the same group

func (rr roundRobinPartitioner) PrepareJoin(jreq *sarama.JoinGroupRequest, topics []string) {
	jreq.AddGroupProtocolMetadata(string(rr),
		&sarama.ConsumerGroupMemberMetadata{
			Version: 1,
			Topics:  topics,
		})
}

// for each topic in jresp, assign the topic's partitions round-robin across the members requesting each topic
func (roundRobinPartitioner) Partition(sreq *sarama.SyncGroupRequest, jresp *sarama.JoinGroupResponse, client sarama.Client) error {
	by_member, err := jresp.GetMembers()
	dbgf("by_member %v", by_member)
	if err != nil {
		return err
	}
	// invert the data, so we have the requests grouped by topic (they arrived grouped by member, since the kafka broker treats the data from each consumer as an opaque blob, so it couldn't do this step for us)
	by_topic := make(map[string][]string) // map of topic to members requesting the topic
	for member, request := range by_member {
		if request.Version != 1 {
			// skip unsupported versions. we'll only assign to clients we can understand. Since we are such a client
			// we won't block all consumers (at least for those topics we consume). If this ends up a bad idea, we
			// can always change this code to return an error.
			continue
		}
		for _, topic := range request.Topics {
			by_topic[topic] = append(by_topic[topic], member)
		}
	}
	dbgf("by_topic %v", by_topic)

	// finally, build our assignments of partitions to members
	assignments := make(map[string]map[string][]int32) // map of member to topics, and topic to partitions
	for topic, members := range by_topic {
		partitions, err := client.Partitions(topic)
		dbgf("Partitions(%q) = %v", topic, partitions)
		if err != nil {
			// what to do? we could maybe skip the topic, assigning it to no-one. But I/O errors are likely to happen again.
			// so let's stop partitioning and return the error.
			return err
		}
		n := len(partitions)
		if n == 0 { // can this happen? best not to /0 later if it can
			// no one gets anything assigned. it is as if this topic didn't exist
			continue
		}

		for i := 0; i < n; {
			for _, member_id := range members {
				topics, ok := assignments[member_id]
				if !ok {
					topics = make(map[string][]int32)
					assignments[member_id] = topics
				}
				topics[topic] = append(topics[topic], partitions[i])
				i++
				if i == n {
					break
				}
			}
		}
	}
	dbgf("assignments %v", assignments)

	// and encode the assignments in the sync request
	for member_id, topics := range assignments {
		sreq.AddGroupAssignmentMember(member_id,
			&sarama.ConsumerGroupMemberAssignment{
				Version: 1,
				Topics:  topics,
			})
	}

	return nil
}

func (roundRobinPartitioner) ParseSync(sresp *sarama.SyncGroupResponse) (map[string][]int32, error) {
	if len(sresp.MemberAssignment) == 0 {
		// in the corner case that we ask for no topics, we get nothing back. However sarama fd498173ae2bf (head of master branch Nov 6th 2016) will return a useless error if we call sresp.GetMemberAssignment() in this case
		return nil, nil
	}
	ma, err := sresp.GetMemberAssignment()
	dbgf("MemberAssignment %v", ma)
	if err != nil {
		return nil, err
	}
	if ma.Version != 1 {
		return nil, fmt.Errorf("unsupported MemberAssignment version %d", ma.Version)
	}
	return ma.Topics, nil
}
