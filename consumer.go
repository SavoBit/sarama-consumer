/*
  A simple kafka consumer-group client

  Copyright 2016 MistSys
*/

package consumer

import (
	"fmt"
	"sync"
	"time"

	"github.com/Shopify/sarama"
)

// minimum kafka API version required. Use this when constructing the sarama.Client's sarama.Config.MinVersion
var MinVersion = sarama.V0_9_0_0

// The errors generated by this package
type Error struct {
	Err     error
	Context string
	cl      *client
}

func (err Error) Error() string {
	return fmt.Sprintf("consumer-group %q: Error %s: %s", err.cl.group_name, err.Context, err.Err)
}

// Config of a Client
type Config struct {
	Offsets struct {
		// The minimum interval between offset commits (defaults to 1s)
		Interval time.Duration
		Retry    struct {
			// The number of retries when comitting offsets (defaults to 3).
			Max int
		}
	}
	Session struct {
		// The allowed session timeout for registered consumers (defaults to 30s).
		// Must be within the allowed server range.
		Timeout time.Duration
	}
	Rebalance struct {
		// The allowed rebalance timeout for registered consumers (defaults to 30s).
		// Must be within the allowed server range. Only functions if sarama.Config.Version >= 0.10.1
		// Otherwise Session.Timeout is used for rebalancing too.
		Timeout time.Duration
	}
	Heartbeat struct {
		// Interval between each heartbeat (defaults to 3s). It should be no more
		// than 1/3rd of the Group.Session.Timout setting
		Interval time.Duration
	}
}

func NewConfig() *Config {
	cfg := &Config{}
	cfg.Offsets.Interval = 1 * time.Second
	cfg.Offsets.Retry.Max = 3
	cfg.Session.Timeout = 30 * time.Second
	cfg.Rebalance.Timeout = 30 * time.Second
	cfg.Heartbeat.Interval = 3 * time.Second
	return cfg
}

/*
  NewClient creates a new consumer group client on top of an existing
  sarama.Client.

  After this call the contents of config should be treated as read-only.
  config can be nil if the defaults are acceptable.

  The consumer group name is used to match this client with other
  instances running elsewhere, but connected to the same cluster
  of kafka brokers and using the same consumer group name.
*/
func NewClient(group_name string, config *Config, sarama_client sarama.Client) (Client, error) {

	cl := &client{
		client:     sarama_client,
		group_name: group_name,
		config:     config,

		closed: make(chan struct{}),
		rejoin: make(chan struct{}, 1),

		consumers: make(map[string]*consumer),
	}

	// start the client's manager goroutine
	rc := make(chan error)
	go cl.run(rc)

	return cl, <-rc
}

/*
  Client is a kafaka client belonging to a consumer group.
*/
type Client interface {
	// Consume returns a consumer of the given topic
	Consume(topic string) (Consumer, error)

	// Close closes the client. It must be called to shutdown
	// the client after AsyncClose is complete in consumers.
	// It does NOT close the inner sarama.Client.
	Close()

	// TODO custom balancer here

	// TODO have a Status() method for debug/logging?
}

/*
  Consumer is a consumer of a topic.

  Messages from any partition assigned to this client arrive on the
  Messages channel, and errors arrive on the Errors channel. These operate
  the same as Messages and Errors in sarama.PartitionConsumer, except
  that messages and errors from any partition are mixed together.

  Every message read from the Messages channel must be eventually passed
  to Done. Calling Done is the signal that that message has been consumed
  and the offset of that message can be comitted back to kafka.

  Of course this requires that the message's Partition and Offset fields not
  be altered.
*/
type Consumer interface {
	// Messages returns the channel of messages arriving from kafka. It always
	// returns the same result, so it is safe to call once and store the result.
	// Every message read from the channel should be passed to Done when processing
	// of the message is complete.
	Messages() <-chan *sarama.ConsumerMessage

	// Done indicates the processing of the message is complete, and its offset can
	// be comitted to kafka. Calling Done twice with the same message, or with a
	// garbage message, can cause panics.
	Done(*sarama.ConsumerMessage)

	// Errors returns the channel of errors. These include errors from the underlying
	// partitions, as well as offset commit errors. Note that sarama's Config.Consumer.Return.Errors
	// is false by default, and without that most errors that occur within sarama are logged rather
	// than returned.
	Errors() <-chan error // TODO consider a custom error type that embodies the source

	// AsyncClose terminates the consumer cleanly. Callers should continue to read from
	// Messages and Errors channels until they are closed. You must call AsyncClose before
	// closing the underlying sarama.Client.
	AsyncClose()
}

// client implements the Client interface
type client struct {
	client     sarama.Client // the sarama client we were constructed from
	config     *Config       // our configuration (read-only)
	group_name string        // the client-group name

	closed chan struct{} // channel which is closed when the client is Close()ed
	rejoin chan struct{} // channel used to force a rejoin of the client, after the topics change

	lock      sync.Mutex           // protects consumers
	consumers map[string]*consumer // the consumers, indexed by topic
}

func (cl *client) Consume(topic string) (Consumer, error) {
	con := &consumer{
		client:   cl,
		topic:    topic,
		messages: make(chan *sarama.ConsumerMessage),
		errors:   make(chan error),
	}

	var err error
	cl.lock.Lock()
	if _, ok := cl.consumers[topic]; !ok {
		cl.consumers[topic] = con
	} else {
		// topic already is being consumed. the way the standard kafka 0.9 group coordination works you cannot consume twice with the
		// same client. If you want to consume the same topic twice, use two Clients.
		con = nil
		err = cl.makeError("Consume", fmt.Errorf("topic %q is already being consumed", topic))
	}
	cl.lock.Unlock()

	// indicate to cl.run() that it should rejoin the group because the configuration has changed
	if con != nil {
		cl.triggerRejoin()
	}

	return con, err
}

func (cl *client) Close() {
	// signal to cl.run() that it should exit
	close(cl.closed)
}

func (cl *client) triggerRejoin() {
	// if there isn't a pending request, send one. otherwise there is already a pending rejoin request
	// and another would be redundant
	select {
	case cl.rejoin <- struct{}{}:
	default:
	}
}

// long lived goroutine which manages this client
func (cl *client) run(early_rc chan<- error) {
	var member_id string // our group member id, assigned to us by kafka when we first make contact
	refresh_coor := false
	pause := false
join_loop:
	for {
		if pause {
			// pause before continuing, so we don't fail continuously too fast
			pause = false
			select {
			case <-time.After(time.Second): // TODO should we increase timeouts?
			case <-cl.closed:
				return
			}
		}
		if refresh_coor {
			refresh_coor = false
			err := cl.client.RefreshCoordinator(cl.group_name)
			if err != nil {
				// TODO where to deliver err?
				pause = true
				continue join_loop
			}
		}

		// make contact with the kafka broker coordinating this group
		// NOTE: sarama keeps the result cached, so we aren't taking a round trip to the kafka brokers very time
		// (then again we need to manage sarama's cache too)
		coor, err := cl.client.Coordinator(cl.group_name)
		if err != nil {
			if early_rc != nil {
				early_rc <- cl.makeError("contacting coordinating broker", err)
				return
			}
			// TODO where to deliver err?

			pause = true
			continue join_loop
		}

		// drain any pending rejoin requests, since we're about to gather up the current state and send a JoinGoupRequest
		select {
		case <-cl.rejoin:
		default:
		}

		// join the group
		jreq := &sarama.JoinGroupRequest{
			GroupId:        cl.group_name,
			SessionTimeout: int32(cl.config.Session.Timeout / time.Millisecond),
			MemberId:       member_id,
			ProtocolType:   "consumer", // we implement the standard kafka 0.9 consumer protocol metadata
		}

		cl.lock.Lock()
		var topics = make([]string, 0, len(cl.consumers))
		for topic := range cl.consumers {
			topics = append(topics, topic)
		}
		cl.lock.Unlock()

		jreq.AddGroupProtocolMetadata("roundrobin", &sarama.ConsumerGroupMemberMetadata{
			Version: 1,
			Topics:  topics,
		})

		jresp, err := coor.JoinGroup(jreq)
		if err != nil || jresp.Err == sarama.ErrNotCoordinatorForConsumer {
			refresh_coor = true // some I/O error happened, or the broker told us it is no longer the coordinator. in either case we should recompute the coordinator
		}
		if err == nil && jresp.Err != 0 {
			err = jresp.Err
		}
		if err != nil {
			err = cl.makeError("joining group", err)
			// if it is still early (the 1st iteration of this loop) then return the error and bail out
			if early_rc != nil {
				early_rc <- err
				return
			}
			// TODO where to deliver err?
			// maybe send it to sarama.Logger?

			pause = true
			continue join_loop
		}

		// we managed to get a successfull join-group response. that is far enough that basic communication is functioning
		// and we can declare that our early_rc is success and release the caller to NewClient
		if early_rc != nil {
			early_rc <- nil
			early_rc = nil
		}

		// save our member_id for next time we join
		member_id = jresp.MemberId

		// TODO map partitions if leader

		// TODO send SyncGroup

		// start the heartbeat timer
		heartbeat_timer := time.After(cl.config.Heartbeat.Interval)

		// and loop, sending heartbeats until something happens and we need to rejoin or exit
	heartbeat_loop:
		for {
			select {
			case <-cl.closed:
				// cl.Close() has been called; time to exit
				resp, err := coor.LeaveGroup(&sarama.LeaveGroupRequest{
					GroupId:  cl.group_name,
					MemberId: jresp.MemberId,
				})
				if err == nil && resp.Err != 0 {
					err = resp.Err
				}
				if err != nil {
					err = cl.makeError("leaving group", err)
				}
				// TODO where to deliver err?

				// and we're done
				return

			case <-cl.rejoin:
				// force a rejoin immediately
				continue join_loop

			case <-heartbeat_timer:
				// send a heartbeat
				resp, err := coor.Heartbeat(&sarama.HeartbeatRequest{
					GroupId:      cl.group_name,
					MemberId:     jresp.MemberId,
					GenerationId: jresp.GenerationId,
				})
				if err != nil || resp.Err == sarama.ErrNotCoordinatorForConsumer {
					refresh_coor = true
				}
				if err != nil || resp.Err != 0 {
					// we've got heartbeat troubles of one kind or another; disconnect and reconnect
					if resp.Err == sarama.ErrNotCoordinatorForConsumer {
						cl.client.RefreshCoordinator(cl.group_name)
					}
					break heartbeat_loop
				}

				// and start the next heartbeat only after we get the response to this one
				// that way when the network or the broker are slow we back off.
				heartbeat_timer = time.After(cl.config.Heartbeat.Interval)
			}
		}
	}
}

// makeError builds an Error from an error from a lower level api (typically the sarama API)
func (cl *client) makeError(context string, err error) error {
	return Error{
		cl:      cl,
		Err:     err,
		Context: context,
	}
}

// consumer implements the Consumer interface
type consumer struct {
	client *client
	topic  string

	messages chan *sarama.ConsumerMessage
	errors   chan error

	closed chan struct{}  // channel which is closed when the consumed is Close()ed
	wg     sync.WaitGroup // waitgroup signaling when all partition consumers have exited
}

func (con *consumer) Messages() <-chan *sarama.ConsumerMessage { return con.messages }
func (con *consumer) Errors() <-chan error                     { return con.errors }

func (con *consumer) Done(*sarama.ConsumerMessage) {
}

func (con *consumer) AsyncClose() {
	close(con.closed)
}

// consumer goroutine
func (con *consumer) run() {
	for {
		select {
		case <-con.closed:
			// wait for the partition consumers to shutdown
			con.wg.Wait()
			// and close the output channels
			close(con.messages)
			close(con.errors)
			// and we're done
			return
		}
	}
}
